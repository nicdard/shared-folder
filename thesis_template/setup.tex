\chapter{System Design}\label{ch:setup}

In this chapter we discuss the overall system design of the project.
First, we provide an architectural overview
of the system and the various components. \cref{sc:architectural-overview}.
We then discuss the requirements of the system from the user point of view
(\cref{sc:real-user}), from the developer point of view
(\cref{sc:developer}) and from the cryptographic point of view
(\cref{sc:abstract-to-real}). We highlight the importance
of taking into account all point of views and stress out the 
relevance of considering the real-world
user for the choice of technologies in the implementation. 

After laying out the requirements, we discuss the various
components of the system, detailing for each of them
the technologies we use. In-depth technical
discussion and technology surveys are provided
to motivate our final choice when required, and we believe
that this is of great interest for the practicioners
that want to implement cryptographic systems with 
a similar technology stack.\footnote{The term ``technology stack'' (or tech stack) generally refers to the set of technologies used together to build a software product.}
However, the reader should feel free to safely skip
them, as they are not strictly necessary to understand 
the rest of this work.

We only discuss common
pieces to both the baseline and the SSF implementation
in this chapter. Most of the system is
shared between the two implementations, to allow
for easy comparison and benchmarking.
The specifics of each implementation are instead discussed in \cref{ch:baseline,ch:ssf}.

\section{Architecture Overview}\label{sc:architectural-overview}

In figure \cref{fig:architecture} we provide an high level overview of the system architecture.
We use different colours to highlight different components of the system, grouping
by frontend, backend, relational databases and cloud components.

\paragraph{Client} The SSF client is a web application that
runs in the browser. The client is used by the user to
interact with the system, and performs the cryptographic
operations needed to access, store and retrieve files 
from the shared folders the user has access to.
Although in the \cref{sc:SSF-scheme} we only model one folder,
in practice the system should support multiple ones.
We imagine the final version of the user interface (UI) to
be similar to the one of Google Drive or Dropbox, where
the user can see the shared folders and navigate inside each
of them to see list of files stored in each of them.
The UI has been kept out of scope from this project, due to
time constraints (\cref{sc:client-overview}).

\paragraph{PKI} The system needs a PKI to manage the identities
of its users. We assume a corporate environment, where
an internal identity provider is available. The PKI server
is a simple certificate authority (CA) which is trusted
by the clients, and issues certificates to users of the system.
The CA certificate is generated once and embedded in the client
code to allow for the verification of other users certificates.
All the certificates are stored in a relational MySQL database \texttt{pki}.

\paragraph{SSF Gateway and DS}
The SSF Gateway server is the main backend component of the system.
The same server will be used also to implement a delivery service (DS), see \cref{sc:MLS} and \cref{ssc:delivery-service}.
These are two different components in abstract, but in practical terms, 
we will see that they share some state (\cref{ssc:delivery-service}).
The SSF Gateway, or simply Gateway, is responsible for the
creation of the shared folders (\cref{sc:SSF-scheme}) and the management of user access
to them. The shared folders are stored in a relational MySQL
database \texttt{ds}, together with the users access control lists (ACLs).
Through the Gateway, files can be uploaded and downloaded 
from the cloud storage provider, which is hidden from the client,
as can be seen in \cref{fig:architecture}.
We can imagine, that the Gateway would constitute the main
server of a company offering a service for secure file sharing.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/architecture.png}
    \caption{System Architecture Overview: The main components of the system are shown with the dependencies between them.}
    \label{fig:architecture}
\end{figure}

\section{High Level Practical Requirements}\label{sc:requirements}

After the initial very high level overview of the system, we
now move on to discuss its requirements instead and continue with a more
detailed explanation of the components of the system in the following sections.

\subsection{Starting with the User in Mind}\label{sc:real-user}

A goal of this thesis (\cref{sc:focus-of-this-thesis})
is to create an MVP. To this end, we start by asking ourselves,
as users of the system, what do we want from it?
\footnote{The study of the use cases and situations from a product 
point of view in industry is normally done through so-called ``user storties''.
User stories as sentences that try to summarise a workflow from the
point of view of a user. They have the following structure:
``As [a user persona] I want [to perform this action] so that [I can accomplish this goal]''.}
Our exploration is limited in time therefore we want to simplify
our requirements to the minimum necessary to run the system
in a real-world setting, 
but still with a customer-centric approach
and leave the possibility to iterate over the initial implementation~\cite{ries2011startup}.\footnote{To be customer-centric is a famous Amazon core approach. Indeed, Amazon has a leadership principle called ``customer obsession''~\cite{AmazonLeadershipPrinciples}.}
We stress out that we do not want to develop just a proof-of-concept of the protocol.

As users, we expect to be able to access 
cloud systems from any device that can navigate online.
This is indeed a core minimum requirement for any modern
cloud storage solution that could make it to the market.
We also think this is a challenge that is worth exploring
in the context of SSF to understand if the underlying
schemes (\cref{ssc:GKP-client-middleware} \cref{sc:background-generalised-DKR})
are actually feasible and applicable for real use cases.
Normally, such ubiquitous access to cloud storage solutions,
is through a web interface.
This in turns means that we need to run our code in the
browser of the user (at least a part of it).

Other user expectations include updates and changes to uploaded files.
Further, we expect our system to provide multi tenancy,
meaning that we handle multiple groups of users at the same time.\footnote{Note that each shared folder is in one to one relationship with the group of users that have access to it.} 
Another side of the multi tenancy aspect, is that a 
single user could be part of multiple such groups at once. 
However, we point out that the SSF scheme (\cref{sc:SSF-scheme}) 
itself is designed for a single shared folder at the time.
Indeed, the underlying primitive GKP (\cref{sc:gkp-scheme}) focus 
on one group at the time in an isolated execution environment.

We highlight that all major file sharing solutions available today meet the above requirements.
Therefore, we claim that these requirements and expectations are expected to be met by any serious attempt to create a new cloud storage solution, including the implementation of the SSF scheme.

\subsection{Cryptography: from Math to Real Execution Environments}\label{sc:abstract-to-real}

Before writing any code, we need to choose the right 
programming language that targets the execution environment
we want to run the code in.

As seen in \cref{ch:background}, the SSF scheme (\cref{sc:SSF-scheme})
uses some advanced cryptographic primitives:
\begin{itemize}
    \item continuous group key agreement (CGKA) (\cref{sc:CGKA})
    \item seekable sequential random generators (SSKG) (\cref{sc:SSKG})
    \item dual-key regression (DKR) (\cref{sc:background-generalised-DKR})
    \item group key progression (GKP) (\cref{sc:gkp-scheme})
\end{itemize}
Implementing these primitives from scratch could in itself
be a complex, time-consuming and error-prone task, especially for CGKA.
It is therefore important to use libraries when available
to deliver reliable implementations.

In our execution environment 
we need support for all cryptographic operations
needed both by the primitives above and by simpler
cryptography which is normally exploited and assumed to exist. 
To be more precise we would need:
\begin{itemize}
    \item Secure random number generation.
    \item Availability and constant time execution of the cryptographic operations underlying the constructions we implement, both for the baseline and the SSF scheme.
    \item Memory safety.
\end{itemize}
The above requirements are needed to avoid security
issues in the implementation. For example, a non-constant
time execution of the cryptographic operations could
lead to timing attacks. 
However, during our implementation work, 
we found that in major runtime
environments, such as the browser, 
support for the cryptographic operations normally
used to instantiate cryptographic primitives is not always available.
This might lead to the usage of custom implementation,
where such guarantees are impossible to provide because
of the lack of low level control on the execution environment.
We detail such findings throughout the rest of the thesis,
and summarise them among other engineering issues in \cref{ch:gaps}.

\subsection{What does the Developer need?}\label{sc:developer}

As developers, we face the challenge of implementing a full
system in a very short time frame and with limited resources.\footnote{To be more precise, the whole implementation has been conducted by only one person in less than six months.}
We want to reduce the possibility of errors and bugs
as well as the complexity in maintaining the integration between
different components.
Sometimes, just a small change in the protocol can lead to
hundreds of lines of changes in the code.
Even more, we want to be able to easily prototype, i.e., 
test out different ideas coming from the theoretical side
or different engineering solutions.
However, changing requirements during the implementation
and updating the system accordingly can take up months of work
if the setup is not properly done to allow development agility.
Further, we would like to easily benchmark the implementations.



\section{The PKI}\label{sc:PKI}

This section presents the PKI server implementation from the architecture overview (\cref{sc:architectural-overview}).

Recalling from the description given in \cref{sc:mental-model},
we need a Public Key Infrastructure (PKI) to implement the SSF scheme.
Papers in cryptography normally assume the existence of a PKI,
which is solving the problem of assigning identities.
Since we want to be able to easily develop the code, and the PKI itself
is not the core problem we are focusing on, we want to rely on existing
solutions.
A common standard way to manage identities are X509 certificates~\cite{rfc5280}.
To implement this solution, we need to have a Certificate Authority (CA)
to distribute the certificates.
Our MVP imaginary target use-case is company or organization. 
Thus, we can assume to have an internal identity provider.
Building on top of X509 certificates, we use transport
layer security (TLS) and mutual TLS (mTLS) to 
secure communication between the clients and the server(s).
While TLS only verifies the server certificate,
in mTLS the client also provides a certificate to the server
for authentication.
The PKI server is using TLS, while the SSF Gateway (\cref{sc:ssf-proxy-server}) is using mTLS.
This way, we greatly simplify development. Indeed, we 
avoid introducing any other Authentication
protocol such as OAuth2 or usage of JWT tokens.
We highlight that this PKI server implementation is not meant for production use.


\paragraph{Implementation Details} 
The PKI server is implemented in Rust, using \texttt{Rocket}
framework for the server and \texttt{sqlx} driver to interact
with the MySQL database. 
For local development, 
the database runs in a Docker container, which allows
for easy setup and teardown.\footnote{Docker is a virtualization technology} 
It is important to have the ability
to easily reset the database to a fresh state 
while developing and testing the system.
The commands to start and stop the container are provided
in the top level \texttt{README.md} of the project.

The code is available in the \texttt{pki} crate
of this project, in \texttt{services/pki} folder.
The server exposes a simple API
to issue certificates to users. 
A client can send a certificate signing request (CSR)
to the server and receive a signed certificate in return.
The code to construct the CSR and issuing certificates, as well as 
verify them is available in the Rust crate \texttt{common},
which is shared among servers and clients (\cref{sc:client-overview}).
We provide unit tests for the library and expose simple bindings (\cref{sc:browser-runtimes}) to be
called from JavaScript in the client (\cref{sc:client-overview}), check the \texttt{README.md}
of the crate for details. All the exposed functionalities are stateless.
When issuing a certificate, validation is not performed
on the user identity, as this is out of scope for the MVP.

The certificates are stored in MySQL ``pki'' database, and the CA
certificate is copied and embedded in the client code to allow
for local verification of other users' certificates.
The server exposes an endpoint to fetch the
CA certificate, which can be used by clients to
refresh the embedded CA certificate.
Further, an endpoint to fetch users' certificates providing the
email of the target user is available.
The description of how to compile and start the server is
provided in the \texttt{README.md} file of the \texttt{pki} crate.
The server is exposed locally at \texttt{\url{https://localhost:8000}}



\paragraph{OpenAPI Specification}
The server API is available in OpenAPI
specification format~\cite{OpenAPISurvey} 
which is generated automatically from the code using
\texttt{utoipa}. Each server endpoint code is
annotated through the library annotations, with the description
of the endpoint, parameters etc. \texttt{utoipa} is then
able to generate the OpenAPI specification in YAML format.
We store the generated file in \texttt{openapi/pki-openapi.yaml}.
We provide an executable written in Rust to perform the generation
through \texttt{utoipa} and regenerate the \texttt{.yaml}
file when the server endpoints are modified.
The OpenAPI specification is then used to generate the
code in TypeScript, which is used in the client to interact
with the PKI server (\cref{sc:client-overview}), thus spearing
the developer from writing thousands of lines of code and keeping them up to date, 
as well as remove reducing the possibility of errors in the client-server interaction. 
The guide on how to use the executable can be found in the \texttt{README.md}
of the crate.
While the server is running, the OpenAPI specification can be
accessed nicely through the Swagger UI from the browser, 
by visiting the page \texttt{\url{http://localhost:8000/swagger/UI}}.

\section{Cloud Storage: from an Abstract Model to Real Systems}\label{sc:cloud-storage}

This section provides the necessary background on cloud storage,
aiming to provide the reader with a first understanding of the challenges
we encounter when dealing with real cloud storage system
as compare to the abstract model we can find in the GKP scheme (\cref{sc:gkp-scheme})~\cite{GKP}.
We start with motivating some of the assumptions that are made
GKP scheme around the cloud storage (\cref{scc:cloud-storage-assumptions}), and then we discuss why
other assumptions are too simplistic and need to be reconsidered.
The discussion of the gaps will also be expanded in more details in the next sections,
where we will use the cloud storage provider to store the shared folders' contents.
The discussion is particularly relevant for the implementation of the
SSF Gateway server (\cref{sc:ssf-proxy-server}).

\subsection{Assumptions}\label{scc:cloud-storage-assumptions}
In the cryptographic research, cloud storage systems are abstracted away by assuming
that some operations are available to write and read data to a virtually
infinite storage system always available.
This is a well-founded assumption, as a cloud storage provider
will normally provision new hardware as needed (in advance) to accommodate the
increased capacity request~\cite{AzureBlobStorage}.

Deletion of files is not assumed to be secure, meaning that it is not guaranteed to happen faithfully and completely,
even if the storage provider is not malicious.
We motivate this assumption with the following facts:
\begin{itemize}
    \item Cloud providers state in their Service Level Agreement (SLA) that
    with very high probability the service is up and running for more than a certain percentage of time or a refund is paid out to the clients.\footnote{e.g.\ Amazon Simple Storage Service (Amazon S3) starts to pay a refund to clients when the monthly uptime percentage is less than 99.9\%}
    The SLA also assures the durability of content uploaded is at least a certain percentage.\footnote{As an example again, Amazon S3 is designed for 99.999999999\% durability}
    \item To meet the above SLA, cloud providers replicate the content. The content is replicated at least (normally) 3 times, and possibly in different geographical zones, to protect against widespread failures~\cite{AzureBlobStorage}.
    \item The content is automatically monitored and re-replicated in case some storage devices are failing.
    \item It is well known that in most case deleting a file from a disk doesn't delete the content but only marks that disk space as free without zeroing out the bits.
    \item The replicated storage could be eventual-consistent, 
    meaning that in case of a network partition the two 
    parts of the system continue to work independently. 
    In this setting, if while the network is partitioned 
    a side of the storage receives a deletion operation 
    for a certain file, upon restoration,
    the nodes still owning the data would try to replicate 
    them to the other side.
    Therefore, instead of deleting the information, 
    the distributed data store creates a (usually temporary) 
    tombstone record to keep track and eventually perform 
    the deletion on all other nodes as well upon reconciliation.
\end{itemize}

While we show that these assumptions are well-founded,
we highlight the following issues when working with
real cloud storage systems, which will be discussed in the
relative sections:
\begin{itemize}
    \item Cloud storage technologies are of many types.
    Different providers can differ in the way they
    offer similar services.
    When setting up the project we need to choose the right
    technology to use. We also need a way to
    abstract away the specific vendor for the storage
    to assure that our solution is easily portable.
    We introduce and discuss the technology of choice in (\cref{ssc:object-storage})
    \item The cloud storage system is not accessible freely
    by clients. Access to cloud resources is
    granted by cloud providers upon registration and payment
    for the service. A system implementing the SSF scheme
    needs to deal with this aspect, and its implications
    on the system design and in terms of security (\cref{sc:cloud-storage-access-and-billing}).
    \item Abstracting the cloud storage system as read/write
    operations on a virtually infinite storage system does
    not capture the complexity of concurrency and consistency
    issues that arise in real-world systems. Although this
    might not be relevant for security in the SSF scheme,
    it is relevant for the correctness of the system and
    security properties normally expected by users (\cref{sc:ssf-file-changes-sync}).
\end{itemize}

\subsection{Object Storage}\label{ssc:object-storage}
As we need to store the shared folders' contents in the cloud,
we check the available storage solutions.
Cloud providers offer different storage solutions,
each with different characteristics. The main file
storage solutions on the market can be divided into
file systems, object storage and databases.

Databases are not needed in this case,
as we do not want to analyse the data stored in
the cloud. Files are encrypted by the client
before the upload.

File systems offering could be used to implement
our shared folders. However, the SSF scheme does not
assume any hierarchy inside a shared folder.
Also, file systems API are rather more complex
than the abstraction that is considered by the
theoretical construction. File storage is also offered
at more expensive prices than the alternative object storage.

Object storage solutions provide
very simple API that allows to store and retrieve
objects in a flat namespace.
The name object storage derives from the fact that this type
of storage does not handle files and file hierarchies 
as we would expect in a file system. Instead,
files are grouped together as plain list of objects,
inside a top level namespace, called a bucket.\footnote{The name is borrowed from Amazon S3, the first such system.}
This type of storage has all the characteristics that are assumed
for the GKP scheme (\cref{sc:gkp-scheme}) as well as for the SSF scheme (\cref{sc:ssf-scheme}).
Stored files are called objects and have the following characteristics:
\begin{itemize}
    \item An object can be big (e.g.\ in Amazon S3 up to 5 GB in a single chunk).
    \item Objects generally can only be written all at once and update or delete them creates new versions of the entire data. A special case are append-only objects, which can be extended with new data without rewriting the whole object each time, but still disallow in-place modifications of the existing content.
\end{itemize} 

We notice that we do not require the ability to update the content of the files in-place,
as the file is always sent encrypted by the client. Therefore, the server has no visibility 
on the content.
Object storage solutions are normally cheaper than the alternatives
and are designed to be highly available and durable.
The cost is really important when considering the implementation
of a real-world system.

Examples of object storage solutions are Amazon Web Services (AWS)
Simple Storage Service (Amazon S3), Microsoft Azure Blob Storage, Google Cloud Storage (GCS).

In our implementation we will use Amazon S3, as it is the most
popular object storage solution, and it is widely used in the industry.
During development, to avoid incurring costs, we will use LocalStack~\cite{LocalStack},
to emulate Amazon Web Services locally on our machine. The availability
of such a development tool was also determining in the choice
of the cloud storage provider. LocalStack
runs as a Docker container.
To avoid vendor lock-in, we will abstract the cloud storage provider
and allow for multiple storage solutions to be used.\footnote{Vendor lock-in indicates the condition where a customer is dependent on a vendor for products and services, and cannot move to another vendor without substantial costs.}
This is discussed in \cref{sc:ssf-proxy-server}.


\subsection{How do users access shared data?}\label{sc:cloud-storage-access-and-billing}
Cloud storage providers are normally accessed through an
identity system. The clients register to the cloud provider.
Through their accounts they can store and load data.
Normally only the account owning the data itself can access it.
However, some storage solutions allow for world-wide public access 
to the data store owned by one account.\footnote{This is the case of Amazon S3 and all other aforementioned object storage solution.} 
This is not recommended for sensitive data.
A recent empirical study from K. Izhikevich et al.~\cite{izhikevich2023using},
shows that there is a strong interest
in compromising publicly accessible data storages,
especially when relatable to commercial entities.
Indeed they found a correlation between storages named after companies and likelihood of being discovered and attacked.
Hundreds of IP addresses attempt to download, delete, or upload objects including malicious shell scripts. 
Furthermore, actors scan public data stores within 40 minutes of deployment and upload unsolicited content within 10 days.

In case instead where each client have its own account
and we keep the data private,
some practical questions arise:
\begin{itemize}
    \item How to share data between clients?
    \item How to manage the access control?
    \item How to manage the billing?
\end{itemize}
Further, our system should be able to integrate with multiple
cloud storage providers. We do not want to force the users to
create accounts on a specific provider, rather this should be
transparent to them.
\nd{I am not sure if I should talk about the following in a later section, where I present the tech stack, or maybe add an ``overwiew of the architecture'' section before the tech stack? }
To this end, we abstract the cloud storage provider itself
and create a server component, called Gateway, which will manage
the access to the cloud storage provider, acting as a proxy
between clients and the cloud storage provider(s).
The Gateway do not only solve the practical issues above, but also
allows for user-expected (non cryptographically enforced) 
security properties of the system to hold.
For example, by performing access control, the Gateway can ensure
that only members of the group can access the data in their
specific shared folder. Although data is E2EE, it is still
good practice to disallow non members from freely download the data,
considering also that the read operations on the storage are also billed.
Further, by disallowing arbitrary writes, the Gateway
protects the data from being overwritten by a malicious external user,
thus protecting from DDOS attacks.
Although availability is out of scope in the SSF scheme,
it is practical required in any real-world system.


\section{Allow the User to Interact with the System: the Client(s)}\label{sc:client-overview}

This section discusses the client side of the system,
called SSF client in the high level architecture (\cref{sc:architectural-overview}).

\paragraph{The Scope of the MVP Client}
As seen in \cref{sc:real-user}, 
we want our client to ultimately be a web application,
running in a browser.
However, for a first development iteration, we want to
remove the development burden of building a UI, which is
a time-consuming task. We instead want to focus on the
implementation of the cryptographic constructions (\cref{ch:background}). 
Still, we want to implement them is such a way so that we
will be able to just run the same code inside the browser,
which is the final target execution environment.
The browser client setup is showcased
in the \texttt{www} folder. Interested practicioners
should check the respective \texttt{README.md} file
for the instructions on how to compile and run the code.

\paragraph{Development Agility} 
To test out the implementation (\cref{sc:developer}), 
and provide the users of our system
with a first MVP version, we develop a
command line interface (CLI) client.
Internally, the CLI will use the cryptographic constructions'
implementations. This part of the code will be tested to be executable
inside the browser to ensure portability and allow a subsequent fast
migration to the final UI. The code for the CLI
can be found in the \texttt{ssf-client} folder.

\paragraph{Code Portability}
Since we want E2EE guarantees (\cref{sc:SSF-scheme}), the protocol execution needs to happen
in the client device of the user.
This implies that we need to investigate the various options
in terms of runtime support in the browser (and in the CLI runtime) 
to check if we can also execute all required cryptographic 
operations (\cref{sc:abstract-to-real}).
Our code portability requirements brought us to interesting
findings on runtime compatibilities,
detailed in \cref{sc:Web-Crypto-API-implementations:-non-standard-behaviours}
and \cref{sc:MLS-enhancements}.
These subtle differences in the runtime environments
make the implementation of portable and secure 
cryptographic software a challenging, or impossible, 
task.

\paragraph{Execution Runtimes} 
Refer to \cref{sc:browser-runtimes} for an in-depth survey about browser runtimes.
We do not only explore the browser setting, but also explore
portability of the code from and outside the browser.
Libraries written in other languages could be ported and used in the implementation.
In short, two runtime environments are available in the browser:
JavaScript (JS), mostly used in web development, and WebAssembly (Wasm),
a newer execution platform, normally the right choice to achieve better performance
and to port code from other languages to the web. Wasm is normally
used in combination with JS, as it cannot interact directly with
the web page as well as with the browser APIs directly. Furthermore, 
as mentioned above, we also want to use our code to first build a CLI.
Node.js is a natural choice to run JS and Wasm outside the browser,
for our portability needs, as it is based on the same 
JS execution engine as Chrome, V8.
Instead of developing directly in JS, we implement in TypeScript (TS),
a statically typed superset of JS, to avoid common JS pitfalls.
For a full, in-depth explanation see \cref{sc:browser-runtimes}.

\paragraph{Cryptographic Support and Libraries}
In \cref{sc:webcrypto-api} we describe the cryptographic API
available in the browser and its limitations. Since we need to use
such API to perform the cryptographic operations, we will use JS
as the main runtime environment.
We also notice that the same API is also implemented and supported in Node.js.
We survey the available 
implementation to date of CGKA 
(\cref{sc:CGKA}) and MLS (\cref{sc:MLS})
to the best of our knowledge (\cref{sc:CGKA-implementations}).
To be able to use the best available
and browser compatible implementation of this cryptographic
primitive we need to integrate Rust code compiled to Wasm, which is
called from JS in our client code. We remind the reader that
Rust is a memory-safe language.
Thanks to its memory safety guarantees and performance,
as well as its expressive type system which prevents many errors already at compile time,
it is a good choice to write cryptographic code.
The library of choice is mls-rs~\cite{AWSMLSGroup}, developed by AWS Lab, 
and open source on GitHub.\footnote{AWS Lab is a research branch of Amazon Web Services (AWS). GitHub is a famous git-based source code sharing platform. The term ``open source'' refer to the fact that the code is publicly available and, as we will see, anyone can read and modify it. Normally, the code is distributed under a licence, so usage might be restricted.}
The library is licenced under MIT and Apache version 2.0, both
permissive licences that allow for unrestricted commercial use of the code.

\paragraph{Certificate Validation}
As we are already using the Rust to Wasm toolchain, 
we will also use some Rust code shared
with our servers to check certificate validity in the client.
This code is provided in \texttt{src/common} as a Rust library
with both native and Wasm compilation targets (\cref{sc:PKI}).
This reduces the amount of code duplication and interoperability
issues between the client and the server. Interested practicioners
should check the \texttt{README.md} file for the
instructions on how to compile the code and the configuration
in \texttt{Cargo.toml} to allow both native and Wasm compilation
targets. We use conditional
compilation to configure how dependencies are imported
given a compilation target. To allow for local verification
of other users certificates,
we embed the CA certificate in the client code.
For testing purposes, we generate also the servers' certificates
from our testing CA.
We also install the servers' certificates in the client code,
to allow for the verification of the servers' identity
in the Node.js CLI, by including them in our HTTPS calls.
See the \texttt{ssf-client/src/protocol/authentication.ts} file.
To be able to call the servers from the browser client,
we need to install the servers' certificates in the OS
certificate store, so that the browser, while connecting 
using the HTTPS protocol, can verify the servers' identity.

\paragraph{Code Generation}
To interact with the servers, we need to write code to
call all the endpoints they expose.
Instead of writing the code manually, we use the OpenAPI
specification generated from the server annotated code (\cref{sc:PKI})
to generate the TypeScript code to call the endpoints.
We have tried multiple generators, and we
decided to use \texttt{@hey-api/openapi-ts}~\cite{OpenAPITs}.
This generator allows generating clients backed by
different HTTP libraries, we rely on \texttt{Axios}~\cite{OpenAPIAxios}, 
to abstract the HTTP calls and easily port the code
between browser and Node.js clients.
Another motivation for using this generator is that it
correctly generates the TS types for the request parameters
and the responses, thus speeding up the development
and enable fast iteration on the client code when a server
endpoint is modified.
We use the generator to create clients both for the PKI server and the SSF Gateway server.
The generated code is created under the subfolder \texttt{gen/clients}.


In the reminder of the chapter we will discuss various topics
related to the client implementation, more in detail.
As already pointed out before, \cref{sc:browser-runtimes}, \cref{sc:webcrypto-api} \cref{sc:CGKA-implementations}
are meant to provide the reader with detailed information 
on the technologies used in the client implementation.
The \cref{sc:CLI} will provide a detailed description of the CLI client,
the code architecture, the abstraction layers, unit and
integration tests setup, the list of commands and an idea of how
to use them.

\subsection{A Deep Dive in Browser Runtimes}\label{sc:browser-runtimes}

JavaScript (JS) is the primarily runtime available in modern browsers.
JS is a managed language, meaning that the programmer
does not have low level control on the allocation and
deallocation of memory. Rather, the language runtime support does it.
In more detail, the heap-allocated memory
is tracked by a garbage collector (GC), which regularly
checks for unreachable objects and frees the memory allocated
to them. We recall that this doesn't solve memory leaks
problems. Indeed, some memory is constantly leaked
between the activations of the GC. Also, the GC
pauses the execution of the program to perform its work,
and can therefore cause timing issues. However, garbage-collected
languages are normally regarded as memory safe, since
the programmer does not need to deal with memory pointers
and their management.


JS is a dynamically
typed language: the types of variables are not checked
statically. This can lead to bugs that are not caught
until the execution of the program.
To mitigate this issue, TypeScript~\cite{bierman2014understanding} (TS)
commonly replaces JavaScript in new or large codebases.
TS is transpiled to JS, it's a superset of JS
\footnote{Any JS program is also a TS program.}
and is statically typed.
Also, other programming languages such as
Kotlin~\cite{KotlinToJs} can be transpiled to JS.

The three major execution engines for JS are V8~\cite{V8} (Chrome/Chromium),
SpiderMonkey~\cite{SpiderMonkey} (Firefox) and JavaScriptCore~\cite{JavaScriptCore} (Safari).\footnote{Note that Edge is now based on Chromium and therefore
also uses V8.}
Among them, V8 is the underlying engine used in other
common JS execution environments on the server side, 
namely Node.js~\cite{NodeJS} and Deno~\cite{Deno}.


WebAssembly (Wasm)~\cite{Haas2017,WasmSpecification} is an alternative runtime
inside browsers.
The Wasm virtual machine (VM) is available in all major
browsers and can be used to run code either written directly in Wasm
or compiled from other languages.
\footnote{Wasm is also supported in Node.js on the server side, however the
code loading process is slightly different.} 
The latter is the
most common case, with source languages like C/C++, Rust,
Kotlin, Go, etc. C/C++ and Rust are low level languages
which allow for fine-grained control of the memory
and do not rely on GC. Emscripten~\cite{Zakai2011} is the primary 
tool for compiling C/C++ to Wasm, using Clang compiler,
which is based on the Low Level Virtual Machine
(LLVM) architecture~\cite{LLVM2004}.
Rust\footnote{Rustc compiler internally also uses LLVM.} also supports compilation for whole application to Wasm
through Emscripten, using the \texttt{wasm32-unknown-emscripten}
compilation target.
However, currently the preferred way is to use
the \texttt{wasm32-unknown-unknown} target, which
compiles to Wasm directly without the need of Emscripten
and produces smaller binaries.\footnote{Normally,
Emscripten is used to port existing application instead 
of building libraries. It indeed 
provides a large standard library, 
containing things such as TCP sockets, 
file I/O, multithreading, openGL etc. that are needed in
standalone applications.
Rust is a new programming language compared to
old C and C++ therefore not many applications were yet 
written before Wasm was created. 
Thus, the preferred usage of Rust code compiled to Wasm
is to write libraries with bindings exposing the
compiled Wasm module to JS in the Browser.}
The Rust to Wasm standard tool chain includes 
wasm-pack~\cite{WasmPack} and wasm-bindgen~\cite{WasmBindgen}.
By using these, the compilation creates a Wasm 
module together with the JS bindings, the ``glue'' code
needed for interoperability, exporting the functionalities, 
so that are easily callable from JS. The TS type declarations 
for the JS generated code are also created.
While C/C++ and Rust compile down to native code, Kotlin normally executes on the
Java Virtual Machine (JVM). It is a high level language with GC,
but it can be compiled to Wasm thanks to the WasmGC proposal
and its support in common execution environments~\cite{WasmGCProposal, WasmGCinV8}.
Go similarly is a garbage collected language~\cite{GoGarbageCollector}.
However, the compilation to Wasm~\cite{GOWasm} 
includes a GC in the compiled code itself, because WasmGC support doesn't
provide certain assurances that Go code expects from its own GC.\footnote{Specifically, the CG should not move memory around while cleaning the unreachable objects.}
For sake of completeness, we mention also the AssemblyScript~\cite{AssemblyScript} language, 
which is a subset of TS that statically compiles ahead of time to Wasm.
Being a subset of TS, AssemblyScript opens up possibilities for
interoperability and sharing of code between the two languages.
It got attention and traction in the community as web developers 
can write in a familiar syntax and easily compile optimised Wasm.

\subsection{Cryptography in Browsers}\label{sc:webcrypto-api}

There are several libraries in JS that provide cryptographic
operations.
However, those libraries present a lot of issues:
\begin{itemize}
    \item Ensure constant time execution is hard. JS engines are constantly changing and tuning how they optimise code, leading to a variety of ever-changing expectations for how the code will actually run
    \item In JS all numbers are 64-bit double precision floating point as specified in the IEEE 754 standard. This means that representation of integers is exact only until $2^{53} - 1$.
    \item There is no source of randomness suitable for cryptographic applications.
    \item Implementors are skilled JS programmers but not skilled cryptographers. Developers maintaining such libraries could make mistakes, as well as developers using such libraries may not understand the implications in terms of security. Further, as mentioned in \cref{sc:abstract-to-real}, it is better to re-use basic primitive implementation that are likely more robust. 
\end{itemize}

The Web Crypto API, a World Wide Web Consortium (W3C) standard~\cite{WebCryptoAPISpecification}, 
provides basic cryptographic support in browsers. This API should
always be used when cryptographic operations are needed in the browser
environment.

The specification has been implemented by all major browsers
and is accessible from JS. 
Node.js and Deno for server-side JavaScript also implement the specification~\cite{NodeJsWebCryptoAPI, DenoWebCryptoAPI}.
The goal of the specification
is to provide a common interface to the underlying 
cryptographic primitives. Also, it provides calls to
a secure source of randomness. 
The API is asynchronous, and it is implemented by each
browser providing native support. This means that the
operations are executed in constant time and can
utilize the hardware acceleration and advanced
security mechanism otherwise unavailable to JS, as well as
more precise arithmetic.\footnote{For example, Chromium is internally calling BoringSSL for the cryptographic operations~\cite{ChromiumWebCryptoAPIImplementation}}
However, we note that the specification itself doesn't mandate
constant time execution of the operations.
Overall, the specification should aim to 
remove the need of aforementioned cryptographic JS libraries,
but support is missing for some new standard cryptographic objects. 
For example, for elliptic curves based cryptography, currently
the API supports P-256, P-386 and P-512 curves~\cite{WebCryptoAPICurvesSupport}.
Howerver, the ``secure curves''~\cite{WebCryptoAPISecureCurvesDraft,WebCryptoAPISecureCurvesExplainer}
are not yet part of the standard, although already recommended by
the Crypto Forum Research Group (CFRG) from the Internet Research
Task Force (IRTF) in 2016 and 2017~\cite{RFC7748IRTF, RFC8032IRTF}
and made part of the Federal Information Processing Standard (FIPS) in 2023~\cite{SecureCurvesNIST}.

When compiling cryptographic code to Wasm in a browser environment,
all cryptographic operations should be done through bindings to
the Web Crypto API. Wasm itself doesn't provide any constant time
guarantee. However, some research were made to add such semantic
to the language~\cite{CTWasm, gu2023constanttimewasmtimerealtime}
and a Wasm constant-time proposal exists in the Wasm specification
repository~\cite{WasmCTProposal}.

\subsection{CGKA Implementations}\label{sc:CGKA-implementations}

CGKA is a major component of the SSF scheme (\cref{sc:SSF}).
It's also a core component in MLS as seen in \cref{sc:CGKA}.
Indeed it is normally implemented as part of libraries developing
the full MLS specification.
To the best of our knowledge, the open source available libraries
providing MLS/CGKA are:
\begin{itemize}
    \item OpenMLS, available in multiple languages but not production-ready.
    \item Java BouncyCastle includes a CGKA only library.
    \item AWS Lab's Rust library ``mls-rs'', a full implementation of MLS sponsored by Amazon Web Services (AWS). 
\end{itemize}

Other minor implementation are available, but are mostly broken or outdated.
Thanks to the support for Wasm builds, mls-rs can be used in the browser.
Out of all the options available, mls-rs seemed the most promising:
we note that the library is developed by some authors of the MLS IETF
specification itself and is also integrated in the Android Open Source Project
code base.\footnote{\url{https://android.googlesource.com/platform/external/rust/crates/mls-rs/}}
The library provides crypto agility, meaning that the cryptographic
primitives can be easily swapped out for new ones and support for multiple cipher suites is available.
For the Wasm build, the library uses bindings to the Web Crypto API 
(\cref{sc:webcrypto-api}) which act as a cryptographic operations' provider.
As described in the relevant section, 
this way the library assure a safe execution of the underlying
algorithms and a source of entropy inside the Browser. 
The bindings for the Web APIs are available in Rust 
through the ``web-sys'' crate,
which is procedurally generated from WebIDL language
used in the API specifications~\cite{WebSys}, assuring
that they are always up-to-date and correct.\footnote{Crate is the name for a Rust library.}\footnote{WebIDL is the interface description language used to define the Web APIs, describing the data types, interfaces, properties and methods and all other components that make up the API itself.} 

Furthermore, while the GKP scheme (\cref{sc:gkp-scheme})
assumes only the usage of CGKA without MLS (and thus the security proof
is simplified), we note that in practice MLS itself is needed.
More details are given in \cref{ch:ssf}.

In summary, we use mls-rs library to get an implementation
of CGKA (and MLS), compiling the library to Wasm to use it both
in the browser client and in the Node.js CLI client. The library is
safe to use, as the cryptographic operations are executed on top of the Web Crypto API.


\subsection{CLI}\label{CLI}

The CLI client is a command line interface tool running in Node.js.
It is meant to be used for testing purposes and to provide a
first MVP client for the system.




\section{Get Our Hands Dirty: The Tech Stack}

We give a complete overview of the tech stack we use both
for the baseline and the SSF implementation.
We discuss both the client and the server side of the project.
We recommend interested practicioners to look at the
project repositories for more details on the various
configurations.
For each library or component, we also provide a markdown
\textit{README.md} file containing instructions on how to
setup the required dependencies. We also provide commands
to run the code and test it.
Given that strive we for code portability, our setup
is quite complex and showcase in details
different targets and configurations.
The project repository is organised as a monorepo,
meaning that all the code is contained in a single
git repository. Also, the Rust code is kept
together in a Cargo Workspace for easier management.%
\footnote{Cargo is the package manager of Rust.
For a detailed description of a Workspace: \url{https://doc.rust-lang.org/cargo/reference/workspaces.html}}
Overall, the project repository
could be taken as a starting point for new projects that
intend to use a similar tech stack.

\subsection{Servers}\label{ssc:servers}

Servers are developed in Rust. Rust is a rather new mainstream language
and it is primarily intended to be used for low level
programming. However the unique type system of Rust
allows for a very safe and performant code, which doesn't
require GC but still provides memory safety.
Writing the servers in Rust allow us to gain confidence
with the language itself and avoid lots of bugs thanks to the
aforementioned properties.
We also want to leverage the Rust cryptographic
ecosystems, which is very rich. Indeed, the combination of
safety and low level control makes Rust a very good choice
for cryptographic code. In particular, we want to take
advantage of libraries like:
\begin{itemize}
    \item Ring, a ``safe, fast, small cryptographic library using Rust with BoringSSL's cryptography primitives''~\cite{Ring}.
    \item Rustls ``a modern TLS library'' which also can use internally Ring as a crypto provider. Rustls ``provides no unsafe features or obsolete cryptography by default''~\cite{Rustls}.
    \item webpki a crate ``to validate Web PKI (TLS/SSL) certificates''~\cite{WebpkiCrate}.
\end{itemize}
Also, Ring supports compilation to Wasm, which comes handy
when we want to share some of the cryptographic code
between servers and clients. In more detail, when targeting the
browser, we can enable the feature flag \texttt{wasm32\_unknown\_unknown\_js}.
In this way the necessary calls to the Web APIs
to perform some cryptographic operations that would
otherwise require a specific operating environment,
e.g. secure random number generation (\cref{sc:webcrypto-api}).
Furthermore, these libraries have received an independent security audit 
from Cure53, sponsored by the Cloud Native Computing Foundation (CNCF),
Indeed Linkerd, a project under the umbrella of the CNCF, is using the same libraries~\cite{RustlsAudit}.
The audit found this cryptographic stack to be exceptionally high in quality.

Web development frameworks are however not as well established
as for example Spring in Java. Some of them support quite a wide
set of features though. 
We tried multiple ones, Warp~\cite{Warp}, 
Actix~\cite{Actix} and Rocket~\cite{Rocket}.
We ultimately pick Rocket as it is the most feature rich
and has a very good documentation and easily integrates with
SQL database drivers crates like sqlx and diesel.
We use sqlx to reduce the cognitive load, as it has a very
low level and straight-forward API to write SQL queries.
Further, to reduce the amount of client code and time to update
integration between servers and clients, we make use of OpenAPI~\cite{OpenAPISurvey}.
OpenAPI (OAS) is a specification to describe APIs. It supports
multiple tools to generate stub client code or mocked servers from
the specification itself.
Using the utopia crate, we annotate the routes of our servers
to allow automatic generation of the OpenAPI specification as 
a Yaml file. This way we will be able to generate the TypeScript
clients for the servers (\cref{ssc:clients}). Unfortunately,
the crate didn't support the latest specification (version 3.1), 
which supports mTLS at the time of writing.
\footnote{See this Github issue in the project repository: \url{https://github.com/juhaku/utoipa/issues/531}}
Support for version 3.1 of the specification is now available,
\footnote{This is a breaking change: \url{https://github.com/juhaku/utoipa/pull/981}}
but mTLS is still missing.\nd{can be part of Future Work to advance the library version}


We have two servers, both in the \texttt{/services} subfolder.
Under \texttt{/services/pki} we implement the PKI server, a simple certificate authority (CA).
It has minimal set of features needed to issue certificates
and validate them.\footnote{As implementing a full compliant CA server is out of scope for this thesis, we left some items in the backlog for improving our base testing implementation.}
The certificate are signed with the
self-signed root certificate of the CA.
The connections to the PKI are secured with TLS.
The PKI server also uses a MySQL database to store the certificates that were issued to clients.
Docker, a virtualization technology, is used during testing. 
With Docker, we can easily spin up a clean MySQL instance running 
locally in our machine as a container and tear it down when we no longer
need it. Each time we start a new container, the database is loaded
with the initial configuration. 
Client certificates use emails of the issuing client to bind the certificate to
a user identity.
The email is represented in the certificate as a \textit{subject alt name} as specified in~\cite{rfc5280}.
The client certificates are generated using the
\texttt{rcgen} crate, which is part of aforementioned Rustls. 
Parsing is performed through
the \texttt{X509-parser} crate. Some of the code
to generate the client certificate request and to
verify the identity of the client in the client certificate
is shared with the clients. 
The crate \texttt{/common} contains this shared code.
It can be included as a Rust library or
compiled to Wasm and the npm package can be then
imported. To power the Wasm build we allow
conditional compilation of subdependency ring as
described above.
The relevant configuration can be found in
\texttt{/common/Cargo.toml}.

The Gateway server mentioned in \cref{sc:cloud-storage}
is implemented with the same stack. It is keeping
a MySQL database to store users registered in the system and
the state of each shared folder, i.e. the current set of users
that can access it. 
It also connects to the cloud storage provider. 
We use Amazon S3 as the storage solution (\cref{ssc:object-storage}).
However, we use the \texttt{object-store} crate as an abstraction layer
that allows our server to easily switch to another cloud storage,
like Azure Blob Storage or Google Cloud Storage, by just changing
some configuration. The library also handles concurrent writes
to an object through optimistic concurrency control.
This means that when trying to update an object,
the server also sends to the storage the ETag (version)
of the version the update is made upon.
If the version is different, the cloud storage will reject the update.
However, Amazon S3 does not support this behaviour as an atomic operation. 
The server itself needs to read the version,
check and then write the new object.
Therefore, the library simulates this mechanism through the usage
of a locking mechanism, where the locks are written in a
Amazon DynamoDB table.
To avoid billing issues and simplify local development and testing,
we use LocalStack. LocalStack is a virtualized simulation
of the AWS cloud stack. It provides implementation both for Amazon S3
APIs and DynamoDB.
Connections to the Gateway server are secured with mTLS.
This way the server performs also authentication of the clients.
As the Gateway maintains the shared folder state and has visibility
of the group of users that can access a certain shared folder, 
it can also enforce access control.
We imagine the Gateway server as the server component of an
imaginary company that provides SSF as a service to its clients.


\subsection{Clients}\label{scc:clients}

The client code is written in TypeScript and Rust compiled to Wasm.
In particular, Rust is present for the X.509 certificate parsing and
verification that is shared with the server-side.
The CGKA library of choice is ``mls-rs'' from AWS-Lab (\cref{sc:CGKA-implementations}).
These technologies can all run in the browser. Specifically,
the resulting JavaScript and Wasm code can be bundled
together via Webpack, a common tool to prepare
the code for the browser.
The project includes a folder showcasing the setup with
Webpack.
However, to simplify developing and allow fast iterations and easier benchmarking,
we use Node.js as execution environment at first.
We develop therefore a command line interface (CLI) to test the code.
This way, we avoid the creation of a user
interface (UI)
\footnote{The UI is considered out of scope for this thesis given the short time frame, but it is a natural extension to the project.}
while still testing out the core protocol. 
As noted in \cref{sc:browser-runtimes}
Node.js internally uses V8, which allows us to assume
similar results in terms of performance and memory usage
when running the code in Chrome. It also provides
an implementation of the Web Crypto API.
However, we needed to patch the ``mls-rs'' library
to allow compatibility with Node.js.\footnote{We contribute our changes to the main project repository: \url{https://github.com/awslabs/mls-rs/pull/189}}
We only change how the library is binding the Web Crypto API.
We provider a JS snippet to load the Web Crypto API from
\texttt{node:crypto} instead of the \texttt{window.crypto} global
object from the browser. All the Rust code remains
unchanged. The library contains also another JS snippet
to use \texttt{Date.now()} function, which is written in
ES6 syntax, which Node.js doesn't support.
We therefore add a commonJS compliant re-implementation
of the same snippet which is used
when compiling for Node.js.
To test the resulting JavaScript and Wasm code, we use the Jest~\cite{Jest}
framework, targeting Node.js as an execution environment.\footnote{Jest can also run partially simulating the browser environment using \texttt{js-dom}.}
For the Rust code that is compiled to Wasm, we make use of
``wasm-bindgen-test'' to test the resulting Wasm code in 
Node.js, as well as in headless browsers.



\section{SSF Proxy Server}\label{sc:ssf-proxy-server}


\section{SSF File Changes Sync}\label{sc:ssf-file-changes-sync}





