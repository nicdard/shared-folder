\chapter{Setup}\label{ch:setup}

In this chapter we discuss the setup of the project.
We start by introducing the questions that in practical
terms arise when we move from the abstract mathematical
model to a real-world minimal viable product (MVP).
We threfore talk about the requirements of the system
for a real user (\cref{sc:real-user}), together with the 
practical requirements
derived from the theoretical construction (\cref{sc:abstract-to-real}).
We also discuss the requirements we have as developers
to implement the system in a very short time frame
(\cref{sc:developer}).

We then discuss the various technologies (tech stack) 
that we will use for both the baseline and the SSF 
implementation (\cref{sc:sc:tech-stack}).
As we want ultimately to compare the two, we
try to keep the tech stack as close as possible.
We do not go into the specifics of each implementation
which are instead discussed in \cref{ch:baseline,ch:ssf}.

\section{Starting with the User in Mind}\label{sc:real-user}

As the goal of this thesis (\cref{sc:focus-of-this-thesis})
is to create an MVP, we need to start by asking ourselves,
as users of the system, what do we want from it?
\footnote{The study of the use cases and situations from a product 
point of view in industry is normally done through so called ``user storties''.
User stories as sentences that try to summarise a workflow from the
point of view of a user. They have the following structure:
"As [a user persona] I want [to perform this action] so that [I can accomplish this goal].}
Our exploration is limited in time threfore we want to simplify
our requirements to the minimum necessary to run the system
in a real-world setting, 
but still with a customer centric approach
\footnote{This is a famous Amazon core approach and leadrship principle, the ``customer obsession''~\cite{AmazonLeadershipPrinciples}.}
and leave the possibility to iterate over the initial implementation~\cite{ries2011startup}.
We do not want to develop just a proof-of-concept of the protocol.

As users we expect to be able to access 
cloud systems from any device that can navigate online.
We think this is indeed a core minimum requirement for any modern
cloud storage solution that could make it to the market.
We also think this is a challenge that is worth exploring
in the context of SSF to explore if the scheme itself is
actually feasible for real usecases.
In modern times, the easiest way
to provide such a feature is to create a web application.
This in turns means that we need to run our code in the
browser of the user (at least a part of it).
Another requirement is that the shared folder should
allow for updates and changes to the files.
Further, we expect our system to provide multi tenancy,
meaning that we handle multiple groups\footnote{Note that each shared folder is in one to one relationship with the group of users that have access to it.}
of users at the same time. 
A single user could be part of multiple such groups at once. 
All major file sharing solutions available today meet the above requirements.
However, we want to stress out that the SSF scheme itself does not deal with them.
It focus on one group at the time in an abstract execution
environment.

\section{Cryptogaphy: from Math to Real Execution Environments}\label{sc:abstract-to-real}

Among the core choices we need to take when developing
the code, we need to first
decide on the programming language and execution
environment that we are targeting.
As seen in \cref{ch:background}, the SSF scheme (\cref{sc:SSF})
uses some advanced cryptographic primitives:
\begin{itemize}
    \item continuous group key agreement (CGKA)
    \item seekable sequential random generators (SSKG)
    \item dual-key regression (DKR)
\end{itemize}
Writing those primitives from scratch could in itself
be a complex and error prone task, especially for CGKA.
It is therefore important to use a library when available
to deliver a reliable implementation.

Discussing at a lower level, in our execution envrionment 
we need support for all cryptographic operations
needed both by the primitives above and by simpler
cryptography which is normally assumed to exist. 
\nd{I do not like this section too much, I think I am going outside the scope of the thesis, wdyt?}
To be more precise we would need:
\begin{itemize}
    \item secure random number generation
    \item availability and constant time execution of the cryptographic operations underlying the theoretical construction both for the baseline and the SSF scheme (to avoid timing attacks) \nd{This is interesting, as I found out the WebCrypto API spec doesn't mandate for constant time execution, so I can discuss it more in details later}
    \item memory safety \nd{this cannot be solved in JS, but maybe is still worth mentioning we would like to have and then say we tried to find a solution but for now there is none?}
\end{itemize}
The above requirements are needed to avoid security
issues in the implementation. For example, a non-constant
time execution of the cryptographic operations could
lead to timing attacks.

\section{What does the developer need?}\label{sc:developer}

As developers, we face the challenge of implementing a full
system in a very short time frame and with limited resources.\footnote{To be more precise, the whole implementation has been conducted by only one person in less than six months.}
We want to reduce the possibility of errors and bugs
as well as the complexity in maintaining the integration of
different components.
Sometimes, just a small change can lead to
hundred of lines of changes.
Even more, we want to be able to easily prototype i.e.\ 
test out different ideas coming from the theoretical side
or different engineering solutions.
However, changing requirements during the implementation
and updating the system accordingly can take up months of work
if the setup is not properly done to allow development agility.
Further, we would like to easily benchmark the implementations.

\section{Browser Technology Background}\label{sc:tech-stack}

As we have seen in \cref{sc:real-user} we want to
run the code (or part of it) in the browser.
Surely, the protocol execution itself needs to happen
in the client device of the user, as we want E2E guarantees.
This restricts our choices of execution runtimes to
those available in the browser. In \cref{sc:browser-runtimes,sc:webcrypto-api}
we briefly describe the various options available in terms of runtime
support in the browser.
Then we survey the available implementation of CGKA (\cref{sc:CGKA-implementations}).


\subsection{Browser Runtimes}\label{sc:browser-runtimes}

JavaScript (JS) is the primarly runtime available in modern browsers.
JS is a managed language, meaning that the programmer
doesn't have low level control on the allocation and
deallocation of memory, rather the runtime does it.
More in detail, the heap allocated memory
is tracked by a garbage collector (GC), which regularly
checks for unreachable objects and frees the memory allocated
to them. We recall that this doesn't solve memory leaks
problems. Indeed, some memory is constantly leaked
between the activations of the GC. Also, the GC
pauses the execution of the program to perform its work,
and can therefore cause timing issues. JS is a dynamically
typed language: the types of variables are not checked
statically. This can lead to bugs that are not caught
until the execution of the program.
To mitigate this issue, TypeScript~\cite{bierman2014understanding} (TS)
commonly replaces JavaScript in new or large codebases.
TS is transpiled to JS, it's a superset of JS
\footnote{Any JS program is also a TS program.}
and is statically typed.
Also other programming languages such as
Kotlin~\cite{KotlinToJs} can be transpiled to JS.

The three major execution engines for JS are V8~\cite{V8} (Chrome/Chromium),
SpiderMonkey~\cite{SpiderMonkey} (Firefox) and JavaScriptCore~\cite{JavaScriptCore} (Safari).
\footnote{Note that Edge is now based on Chromium and therefore
also uses V8.}
Among them, V8 is the underlying engine used in other
common JS execution environments on the server side, 
namely Node.js~\cite{NodeJS} and Deno~\cite{Deno}.


WebAssembly (Wasm)~\cite{Haas2017,WasmSpecification} is an alternative runtime
inside browsers.
The Wasm virtual machine (VM) is available in all major
browsers and can be used to run code either written directly in Wasm
or compiled from other languages.
\footnote{Wasm is also supported in Node.js on the server side, however the
code loading process is slightly different.} 
The latter is the
most common case, with source languages like C/C++, Rust,
Kotlin, Go, etc. C/C++ and Rust are low level languages
which allow for fine grained control of the memory
and do not rely on GC. Emscripten~\cite{Zakai2011} is the primary 
tool for compiling C/C++ to Wasm, using Clang compiler,
which is based on the Low Level Virtual Machine
(LLVM) architecture~\cite{LLVM2004}.
Rust\footnote{Rustc compiler internally also uses LLVM.} also supports compilation for whole application to Wasm
through Emscripten, using the \texttt{wasm32-unknown-emscripten}
compilation target.
However, currently the preferred way is to use
the \texttt{wasm32-unknown-unknown} target, which
compiles to Wasm directly without the need of Emscripten
and produces smaller binaries.\footnote{Normally,
Emscripten is used to port existing application instead 
of building libraries. It indeed 
provides a large standard library, 
containing things such as TCP sockets, 
file I/O, multithreading, openGL etc. that are needed in
standalone applications.
Rust is a new programming language compared to
old C and C++ therefore not so many application were yet 
written before Wasm was created. 
Thus, the preferred usage of Rust code compiled to Wasm
is to write libraries with bindings exposing the
compiled Wasm module to JS in the Browser.}
The Rust to Wasm standard tool chain includes 
wasm-pack~\cite{WasmPack} and wasm-bindgen~\cite{WasmBindgen}.
By using these, the compilation creates a Wasm 
module together with the JS bindings, the ``glue'' code
needed for interoperability, exporting the functionalities
so that are easily callable from JS. The TS type declarations 
for the JS generated code are also created.
While C/C++ and Rust compile down to native code, Kotlin normally executes on the
Java Virtual Machine (JVM). It is a high level language with GC,
but it can be compiled to Wasm thanks to the WasmGC proposal
and its support in common execution environments~\cite{WasmGCProposal, WasmGCinV8}.
Go similarly is a garbage collected language~\cite{GoGarbageCollector}.
However the compilation to Wasm~\cite{GOWasm} 
includes a GC in the compiled code itself, because WasmGC support doesn't
provide certain assurances that Go code expects from its own GC.\footnote{Specifically, the CG should not move memory around while cleaning the unreachable objects.}
For sake of completeness, we mention also the AssemblyScript~\cite{AssemblyScript} language, 
which is a subset of TS that statically compiles ahead of time to Wasm.
Being a subset of TS, AssemblyScript opens up possibilities for
interoperability and sharing of code between the two languages.
It got attention and traction in the community as web developers 
can write in a familiar syntax and easily compile optimised Wasm.

\subsection{Cryptography in Browsers}\label{sc:webcrypto-api}

There are several libraries in JS to provide cryptographic
operations.
However, those libraries present a lot of issues:
\begin{itemize}
    \item Assure constant time execution is hard. JS engines are constantly changing and tuning how they optimize code, leading to a variety of ever changing expectations for how the code will actually run
    \item In JS all numbers are 64-bit double precision floating point as specified in the IEEE 754 standard. This means that representation of integers is exact until $2^{53} - 1$.
    \item A good source of entropy is missing.
    \item Implementors are skilled JS programmers but not skilled cryptographers. Further, as mentioned in \cref{sc:abstract-to-real}, it is better to re-use basic primitive implementation that are likely more robust. 
\end{itemize}

The Web Crypto API, a World Wide Web Consortium (W3C) standard~\cite{WebCryptoAPISpecification}, 
provides basic cryptographic support in browsers.
The specification has been implemented by all major browsers
and is accessable from JS. 
Node.js and Deno for server-side JavaScript also implement the specification~\cite{NodeJsWebCryptoAPI, DenoWebCryptoAPI}.
The goal of the specification
is to provide a common interface to the underlying 
cryptographic primitives. Also, it provides calls to
a secure source of randomness. 
The API is asynchronous and it is implemented by each
browser providing native support. This means that the
operations are executed in constant time and can
utilize the hardware acceleration and advanced
security mechanism otherwise unavailable to JS, as well as
more precise arithmetic.
\footnote{For example, Chromium is internally calling BoringSSL for the cryptographic operations~\cite{ChromiumWebCryptoAPIImplementation}}
However, we note that the specification itself doesn't mandate
constant time execution of the operations.
Overall, the specification should aim to 
remove the need of aforementioned cryptographic JS libraries,
but support is missing for some new standard cryptographic objects. 
For example, for elliptic curves based cryptography, currently
the API supports P-256, P-386 and P-512 curves~\cite{WebCryptoAPICurvesSupport}.
Howerver, the ``secure curves''~\cite{WebCryptoAPISecureCurvesDraft,WebCryptoAPISecureCurvesExplainer}
are not yet part of the standard, although already recommended by
the Crypto Forum Research Group (CFRG) from the Internet Research
Task Force (IRTF) in 2016 and 2017~\cite{RFC7748IRTF, RFC8032IRTF}
and made part of the Federeal Information Processing Standard (FIPS) in 2023~\cite{SecureCurvesNIST}.

When compiling cryptographic code to Wasm in a browser environment,
all cryptographic operations should be done through bindings to
the Web Crypto API. Wasm itself doesn't provide any constant time
guarantee. However, some research were made to add such semantic
to the language~\cite{CTWasm, gu2023constanttimewasmtimerealtime}
and a Wasm constant-time proposal exists in the Wasm specification
repository~\cite{WasmCTProposal}.

\subsection{CGKA Implementations}\label{sc:CGKA-implementations}

CGKA is a major component of the SSF scheme (\cref{sc:SSF}).
It's also a core component in MLS as seen in \cref{sc:CGKA}.
Indeed it is normally implemented as part of libraries developing
the full MLS specification.
To the best of our knowledge, the open source available libraries
providing MLS/CGKA are:
\begin{itemize}
    \item OpenMLS, available in multiple languages but not production-ready.
    \item Java BouncyCastle includes a CGKA only library.
    \item AWS-Lab Rust library ``mls-rs'', a full implementation of MLS sponsored by Amazon Web Services (AWS). 
\end{itemize}

Other minor implementation are available, but are mostly broken or outdated.
Thanks for the support for Wasm builds, ``mls-rs'' can be used in the browser.
Out of all the options available, ``mls-rs'' seemed the most promising:
we note that the library is developed by some of the authors of the MLS IETF
specification itself and is also integrated in the Android Open Source Project
code base\footnote{\url{https://android.googlesource.com/platform/external/rust/crates/mls-rs/}}.
The library provides crypto agility, meaning that the cryptographic
primitives can be easily swapped out for new ones.
For the Wasm build, the library uses bindings to the Web Crypto API 
(\cref{sc:webcrypto-api}) which act as a cryptographic operations provider.
As mentioned above, this way the library assure a safe execution of the underlying
algorithms and a good source of entropy inside the Browser. 
The bindings for the Web APIs are available in Rust 
through the ``web-sys'' crate,
\footnote{Crate is the name for a Rust library.}
which is procedurally generated from WebIDL language
\footnote{WebIDL is the interface description language used to define the Web APIs, describing the data types, interfaces, properties and methods and all other components that make up the API itself.} 
used in the API specifications~\cite{WebSys}, assuring
that they are up to date and correct.
Furthermore, while the SSF theoretical construction
assumes only the usage of CGKA without MLS (and thus the security proof
is simplified), we note that in practice MLS itself is needed.
More details are given in \cref{ch:ssf}.

\section{Cloud Storage: from an Abstract Model to Real Systems}\label{sc:cloud-storage}

Before explaining in detail the complete setup of the project,
we also need to deal with how to access the cloud storage provider,
which is the other core component in the SSF scheme construction.

In SSF, all cloud storage systems are abstracted away by assuming
that some operations are available to write and read data to a virtually
infinite storage system always available.
Deletion of files is not assumed to be secure, meaning that it is not guaranteed to happen faithfully and completely.
The above assumptions are well founded, given that a cloud storage provider
will normally provision new hardware as needed (in advance) to accomodate the
increased capacity request.
Also, a client writing in a public cloud doesn't have any control of that file anymore.
Furthermore, cloud providers state in their Service Level Agreement (SLA) that
with very high probability the service is up and running for more than a certain percentage of time or a refund is payed out to the clients
\footnote{e.g.\ Amazon Simple Storage Service (Amazon S3) starts to pay a refund to clients when the monthly uptime percentage is less than 99.9\%}
and that the durability of content uploaded is at least a certain percentage.
\footnote{As an example again, Amazon S3 is designed for 99.999999999\% durability}
The way cloud providers can meet those requirements is through replication, where the content is replicated at least (normally) 3 times, and in different geographical zones, to protect against widespread failures.
The content is automatically monitored and re-replicated in case some storage devices are failing. 
Furthermore, it is well known that in most case deleting a file from a disk doesn't delete the content but only marks that disk space as free without zerooing out the bits. 
Other reasons why we cannot assume that cloud storages perform secure deletion, at a higher level, but still caused by the same idea of marking some memory as free without actually deleting it,
happens in replicated storages, if the replicated storage is eventual-consistent, meaning that in case of a network partition the two parts of the system continue to work indipendently.
In such a scenario, if while the network is partitioned a side of the storage receives a deletion operation for a certain file, upon restoration,
the nodes still owning the data would try to replicate them to the other side.
Therefore, instead of deleting the information, the distributed data store creates a (usually temporary) tombstone record to keep track and eventually perform the deletion on all other nodes as well upon reconciliation.

\subsection{Object Storage}\label{ssc:object-storage}
Examples of object storage solutions are Amazon Web Services (AWS)
Simple Storage Service (Amazon S3), Microsoft Azure Blob Storage, Google Cloud Storage (GCS).
This type of storage has all the characteristics aforementioned and can 
therefore be the actual technology used in the implementation. 
Further, it is called object storage as it deals with objects and not files, 
where an object can be very large (e.g.\ in Amazon S3 up to 5GB).
However, objects can only be written all at once
and update or delete them creates new versions of the entire data.
Compared to a normal file system, object storage has therefore a simpler API.

Other solutions could be used to implement the storage layer,
like a distributed file system.
However, object storage seems to be the most appropriate
solution as data is written encrypted by the client and therefore
there is no concept of file in-place modification.
Also, cost wise, object storage is normally very cheap.

\subsection{How do users access shared data?}
Cloud storage providers are normally accessed through an
identity system. The clients register to the cloud provider and
through their accounts they can store and load data.
Normally only accounts owning the data itself can access it.
However, some storage solutions allow for world-wide public access 
to the data store owned by one account,
\footnote{This is the case of Amazon S3 and all other aforementioned object storage solution.} 
although this is not recommended for sensitive data.
A recent empirical study from K. Izhikevich et al.~\cite{izhikevich2023using},
shows that there is a strong interest
in compromising publicly accessible data storages,
especially when relatable to commercial entities.
Indeed they found a correlation between storages named after companies and likelihood of being discovered and attacked.
Hundreds of IP addresses attempt to download, delete, or upload objects including malicious shell scripts. 
Furthermore, actors scan public data stores within 40 minutes of deployment and upload unsolicited content within 10 days.

In case instead where each client have its own account
and we keep the data private,
some practical questions arise:
\begin{itemize}
    \item How to share data between clients?
    \item How to manage the access control?
    \item How to manage the billing?
\end{itemize}
Further, our system should be able to integrate with multiple
cloud storage providers. We do not want to force the users to
create accounts on a specific provider, rather this should be
transparent to them.
\nd{I am not sure if I should talk about the following in a later section, where I present the tech stack, or maybe add an ``overwiew of the architecture'' section before the tech stack? }
To this end, we abstract the cloud storage provider itself
and create a server component, called Gateway, which will manage
the access to the cloud storage provider, acting as a proxy
between clients and the cloud storage provider(s).
The Gateway do not only solve the practical issues above, but also
allows for user-expected (non cryptographically enforced) 
security properties of the system to hold.
For example, by performing access control, the Gateway can ensure
that only members of the group can access the data in their
specific shared folder. Although data is E2EE, it is still
good practice to disallow non members from freely download the data,
considering also that the read operations on the storage are also billed.
Further, by disallowing arbitrary writes, the Gateway
protects the data from being overwritten by a malicious external user,
thus protecting from DDOS attacks.
Although availability is out of scope in the SSF scheme,
it is practical required in any real-world system.

\section{The PKI}

Recalling from the description given in \cref{sc:mental-model},
the last missing piece we need to consider before moving
on the tehcnology choice is the Public Key Infrastructure (PKI).
Papers in cryptography normally assume the existance of a PKI,
which is solving the identities problem.
Since we want to be able to easily develop the code, and the PKI itself
is not the core problem we are focusing on, we want to rely on existing
solutions.
A common standard way to manage identities are X509 certificates~\cite{rfc5280}.
To implement this solution, we need to have a Certificate Authority (CA)
to distribute the certificates.
Our MVP ideally targets a company or organization as a real use-case. 
Thus, we can assume to have an internal identity provider.
Building on top of X509 certificates, we can use transport
layer security (TLS) and mutual TLS (mTLS) to 
secure communication between the clients and the server(s). 
While TLS only authenticates verifies the server certificate,
in mTLS the client also provides a certificate to the server
for authentication.
This way, we greatly simplify development. Indeed we 
avoid introducing any other Authentication
protocol such as OAuth2 or usage of JWT tokens.

\section{Get Our Hands Dirty: The Tech Stack}

We give a complete overview of the tech stack we use both
for the baseline and the SSF implementation.
We discuss both the client and the server side of the project.
We recommend interested practicioners to look at the
project repositories for more details on the various
configurations.
For each library or component, we also provide a markdown
\textit{README.md} file containing instructions on how to
setup the required dependencies. We also provide commands
to run the code and test it.
Given that strive we for code portability, our setup
is quite complex and showcase in details
different targets and configurations.
The project repository is organised as a monorepo,
meaning that all the code is contained in a single
git repository. Also, the Rust code is kept
together in a Cargo Workspace for easier management.%
\footnote{Cargo is the package manager of Rust.
For a detailed description of a Workspace: \url{https://doc.rust-lang.org/cargo/reference/workspaces.html}}
Overall, the project repository
could be taken as a starting point for new projects that
intend to use a similar tech stack.

\subsection{Servers}\label{ssc:servers}

Servers are developed in Rust. Rust is a rather new mainstream language
and it is primarily intended to be used for low level
programming. However the unique type system of Rust
allows for a very safe and performant code, which doesn't
require GC but still provides memory safety.
Writing the servers in Rust allow us to gain confidence
with the language itself and avoid lots of bugs thanks to the
aforementioned properties.
We also want to leverage the Rust cryptographic
ecosystems, which is very rich. Indeed, the combination of
safety and low level control makes Rust a very good choice
for cryptographic code. In particular, we want to take
advantage of libraries like:
\begin{itemize}
    \item Ring, a ``safe, fast, small cryptographic library using Rust with BoringSSL's cryptography primitives''~\cite{Ring}.
    \item Rustls ``a modern TLS library'' which also can use internally Ring as a crypto provider. Rustls ``provides no unsafe features or obsolete cryptography by default''~\cite{Rustls}.
    \item webpki a crate ``to validate Web PKI (TLS/SSL) certificates''~\cite{WebpkiCrate}.
\end{itemize}
Also, Ring supports compilation to Wasm, which comes handy
when we want to share some of the cryptographic code
between servers and clients. In more detail, when targeting the
browser, we can enable the feature flag \texttt{wasm32\_unknown\_unknown\_js}.
In this way the necessary calls to the Web APIs
to perform some cryptographic operations that would
otherwise require a specific operating environment,
e.g. secure random number generation (\cref{sc:webcrypto-api}).
Furthermore, these libraries have received an independent security audit 
from Cure53, sponsored by the Cloud Native Computing Foundation (CNCF),
Indeed Linkerd, a project under the umbrella of the CNCF, is using the same libraries~\cite{RustlsAudit}.
The audit found this cryptographic stack to be exceptionally high in quality.

Web development frameworks are however not as well established
as for example Spring in Java. Some of them support quite a wide
set of features though. 
We tried multiple ones, Warp~\cite{Warp}, 
Actix~\cite{Actix} and Rocket~\cite{Rocket}.
We ultimately pick Rocket as it is the most feature rich
and has a very good documentation and easily integrates with
SQL database drivers crates like sqlx and diesel.
We use sqlx to reduce the cognitive load, as it has a very
low level and straight-forward API to write SQL queries.
Further, to reduce the amount of client code and time to update
integration between servers and clients, we make use of OpenAPI~\cite{OpenAPISurvey}.
OpenAPI (OAS) is a specification to describe APIs. It supports
multiple tools to generate stub client code or mocked servers from
the specification itself.
Using the utopia crate, we annotate the routes of our servers
to allow automatic generation of the OpenAPI specification as 
a Yaml file. This way we will be able to generate the TypeScript
clients for the servers (\cref{ssc:clients}). Unfortunately,
the crate didn't support the latest specification (version 3.1), 
which supports mTLS at the time of writing.
\footnote{See this Github issue in the project repository: \url{https://github.com/juhaku/utoipa/issues/531}}
Support for version 3.1 of the specification is now available,
\footnote{This is a breaking change: \url{https://github.com/juhaku/utoipa/pull/981}}
but mTLS is still missing.\nd{can be part of Future Work to advance the library version}


We have two servers, both in the \texttt{/services} subfolder.
Under \texttt{/services/pki} we implement the PKI server, a simple certificate authority (CA).
It has minimal set of features needed to issue certificates
and validate them.\footnote{As implementing a full compliant CA server is out of scope for this thesis, we left some items in the backlog for improving our base testing implementation.}
The certificate are signed with the
self-signed root certificate of the CA.
The connections to the PKI are secured with TLS.
The PKI server also uses a MySQL database to store the certificates that were issued to clients.
Docker, a virtualization technology, is used during testing. 
With Docker, we can easily spin up a clean MySQL instance running 
locally in our machine as a container and tear it down when we no longer
need it. Each time we start a new container, the database is loaded
with the initial configuration. 
Client certificates use emails of the issuing client to bind the certificate to
a user identity.
The email is represented in the certificate as a \textit{subject alt name} as specified in~\cite{rfc5280}.
The client certificates are generated using the
\texttt{rcgen} crate, which is part of aforementioned Rustls. 
Parsing is performed through
the \texttt{X509-parser} crate. Some of the code
to generate the client certificate request and to
verify the identity of the client in the client certificate
is shared with the clients. 
The crate \texttt{/common} contains this shared code.
It can be included as a Rust library or
compiled to Wasm and the npm package can be then
imported. To power the Wasm build we allow
conditional compilation of subdependency ring as
described above.
The relevant configuration can be found in
\texttt{/common/Cargo.toml}.

The Gateway server mentioned in \cref{sc:cloud-storage}
is implemented with the same stack. It is keeping
a MySQL database to store users registered in the system and
the state of each shared folder, i.e. the current set of users
that can access it. 
It also connects to the cloud storage provider. 
We use Amazon S3 as the storage solution (\cref{ssc:object-storage}).
However, we use the \texttt{object-store} crate as an abstraction layer
that allows our server to easily switch to another cloud storage,
like Azure Blob Storage or Google Cloud Storage, by just changing
some configuration. The library also handles concurrent writes
to an object through optimistic concurrency control.
This means that when trying to update an object,
the server also sends to the storage the ETag (version)
of the version the update is made upon.
If the version is different, the cloud storage will reject the update.
However, Amazon S3 does not support this behaviour as an atomic operation. 
The server itself needs to read the version,
check and then write the new object.
Therefore, the library simulates this mechanism through the usage
of a locking mechanism, where the locks are written in a
Amazon DynamoDB table.
To avoid billing issues and simplify local development and testing,
we use LocalStack. LocalStack is a virtualized simulation
of the AWS cloud stack. It provides implementation both for Amazon S3
APIs and DynamoDB.
Connections to the Gateway server are secured with mTLS.
This way the server performs also authentication of the clients.
As the Gatway maintains the shared folder state and has visibility
of the group of users that can access a certain shared folder, 
it can also enforce access control.
We imagine the Gateway server as the server component of an
imaginary company that provides SSF as a service to its clients.


\subsection{Clients}\label{scc:clients}

The client code is written in TypeScript and Rust compiled to Wasm.
In particular, Rust is present for the X.509 certificate parsing and
verification that is shared with the server-side.
The CGKA library of choice is ``mls-rs'' from AWS-Lab (\cref{sc:CGKA-implementations}).
Those technology can all run in the browser. Specifically,
the resulting JavaScript and Wasm code can be bundled
together via Webpack, a common tool to prepare
the code for the browser.
The project includes a folder showcasing the setup with
Webpack.
However, to simplify developing and allow fast iterations and easier benchmarking,
we use Node.js as execution environment at first.
We develop therefore a command line interface (CLI) to test the code.
This way, we avoid the creation of a user
interface (UI)
\footnote{The UI is considered out of scope for this thesis given the short time frame, but it is a natural extension to the project.}
while still testing out the core protocol. 
As noted in \cref{sc:browser-runtimes}
Node.js internally uses V8, which allows us to assume
similar results in terms of performance and memory usage
when running the code in Chrome. It also provides
an implementation of the Web Crypto API.
However, we needed to patch the ``mls-rs'' library
to allow compatibility with Node.js.\footnote{We contribute our changes to the main project repository: \url{https://github.com/awslabs/mls-rs/pull/189}}
We only change how the library is binding the Web Crypto API.
We provider a JS snippet to load the Web Crypto API from
\texttt{node:crypto} instead of the \texttt{window.crypto} global
object from the browser. All the Rust code remains
unchanged. The library contains also another JS snippet
to use \texttt{Date.now()} function, which is written in
ES6 syntax, which Node.js doesn't support.
We therefore add a commonJS compliant re-implementation
of the same snippet which is used
when compiling for Node.js.
To test the resulting JavaScript and Wasm code, we use the Jest~\cite{Jest}
framework, targeting Node.js as an execution environment.\footnote{Jest can also run partially simulating the browser environment using \texttt{js-dom}.}
For the Rust code that is compiled to Wasm, we make use of
``wasm-bindgen-test'' to test the resulting Wasm code in 
Node.js, as well as in headless browsers.






