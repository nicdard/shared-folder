\chapter{Setup}\label{ch:setup}

In this chapter we discuss the setup of the project.
We start by introducing the questions that in practical
terms arise when we move from the abstract mathematical
model to a real-world minimal viable product (MVP).
We threfore talk about the requirements of the system
for a real user (\cref{sc:real-user}), together with the 
practical requirements
derived from the theoretical construction (\cref{sc:abstract-to-real}).
We also discuss the requirements we have as developers
to implement the system in a very short time frame
(\cref{sc:developer}).

We then discuss the various technologies (tech stack) 
that we will use for both the baseline and the SSF 
implementation (\cref{sc:sc:tech-stack}).
As we want ultimately to compare the two, we
try to keep the tech stack as close as possible.
We do not go into the specifics of each implementation
which are instead discussed in \cref{ch:baseline,ch:ssf}.

\section{Starting with the User in Mind}\label{sc:real-user}

As the goal of this thesis (\cref{sc:focus-of-this-thesis})
is to create an MVP, we need to start by asking ourselves,
as users of the system, what do we want from it?
\footnote{The study of the use cases and situations from a product 
point of view in industry is normally done through so called ``user storties''.
User stories as sentences that try to summarise a workflow from the
point of view of a user. They have the following structure:
"As [a user persona] I want [to perform this action] so that [I can accomplish this goal].}
Our exploration is limited in time threfore we want to simplify
our requirements to the minimum necessary to run the system
in a real-world setting, 
but still with a customer centric approach
\footnote{This is a famous Amazon core approach and leadrship principle, the ``customer obsession''~\cite{AmazonLeadershipPrinciples}.}
and leave the possibility to iterate over the initial implementation~\cite{ries2011startup}.
We do not want to develop just a proof-of-concept of the protocol.

As users we expect to be able to access 
cloud systems from any device that can navigate online.
We think this is indeed a core minimum requirement for any modern
cloud storage solution that could make it to the market.
We also think this is a challenge that is worth exploring
in the context of SSF to explore if the scheme itself is
actually feasible for real usecases.
In modern times, the easiest way
to provide such a feature is to create a web application.
This in turns means that we need to run our code in the
browser of the user (at least a part of it).
Another requirement is that the shared folder should
allow for updates and changes to the files.
Further, we expect our system to provide multi tenancy,
meaning that we handle multiple groups\footnote{Note that each shared folder is in one to one relationship with the group of users that have access to it.}
of users at the same time. 
A single user could be part of multiple such groups at once. 
All major file sharing solutions available today meet the above requirements.
However, we want to stress out that the SSF scheme itself does not deal with them.
It focus on one group at the time in an abstract execution
environment.

\section{Cryptogaphy: from Math to Real Execution Environments}\label{sc:abstract-to-real}

Among the core choices we need to take when developing
the code, we need to first
decide on the programming language and execution
environment that we are targeting.
As seen in \cref{ch:background}, the SSF scheme (\cref{sc:SSF})
uses some advanced cryptographic primitives:
\begin{itemize}
    \item continuous group key agreement (CGKA)
    \item seekable sequential random generators (SSKG)
    \item dual-key regression (DKR)
\end{itemize}
Writing those primitives from scratch could in itself
be a complex and error prone task, especially for CGKA.
It is therefore important to use a library when available
to deliver a reliable implementation.

Discussing at a lower level, in our execution envrionment 
we need support for all cryptographic operations
needed both by the primitives above and by simpler
cryptography which is normally assumed to exist. 
\nd{I do not like this section too much, I think I am going outside the scope of the thesis, wdyt?}
To be more precise we would need:
\begin{itemize}
    \item secure random number generation
    \item availability and constant time execution of the cryptographic operations underlying the theoretical construction both for the baseline and the SSF scheme (to avoid timing attacks) \nd{This is interesting, as I found out the WebCrypto API spec doesn't mandate for constant time execution, so I can discuss it more in details later}
    \item memory safety \nd{this cannot be solved in JS, but maybe is still worth mentioning we would like to have and then say we tried to find a solution but for now there is none?}
\end{itemize}
The above requirements are needed to avoid security
issues in the implementation. For example, a non-constant
time execution of the cryptographic operations could
lead to timing attacks.

\section{What does the developer need?}\label{sc:developer}

As developers, we face the challenge of implementing a full
system in a very short time frame and with limited resources.\footnote{To be more precise, the whole implementation has been conducted by only one person in less than six months.}
We want to reduce the possibility of errors and bugs
as well as the complexity in maintaining the integration of
different components.
Sometimes, just a small change can lead to
hundred of lines of changes.
Even more, we want to be able to easily prototype i.e.\ 
test out different ideas coming from the theoretical side
or different engineering solutions.
However, changing requirements during the implementation
and updating the system accordingly can take up months of work
if the setup is not properly done to allow development agility.
Further, we would like to easily benchmark the implementations.

\section{Browser Technology Background}\label{sc:tech-stack}

As we have seen in \cref{sc:real-user} we want to
run the code (or part of it) in the browser.
Surely, the protocol execution itself needs to happen
in the client device of the user, as we want E2E guarantees.
This restricts our choices of execution runtimes to
those available in the browser. In \cref{sc:browser-runtimes,sc:webcrypto-api}
we briefly describe the various options available in terms of runtime
support in the browser.
Then we survey the available implementation of CGKA (\cref{sc:CGKA-implementations}).


\subsection{Browser Runtimes}\label{sc:browser-runtimes}

JavaScript (JS) is the primarly runtime available in modern browsers.
JS is a managed language, meaning that the programmer
doesn't have low level control on the allocation and
deallocation of memory, rather the runtime does it.
More in detail, the heap allocated memory
is tracked by a garbage collector (GC), which regularly
checks for unreachable objects and frees the memory allocated
to them. We recall that this doesn't solve memory leaks
problems. Indeed, some memory is constantly leaked
between the activations of the GC. Also, the GC
pauses the execution of the program to perform its work,
and can therefore cause timing issues. JS is a dynamically
typed language: the types of variables are not checked
statically. This can lead to bugs that are not caught
until the execution of the program.
To mitigate this issue, TypeScript~\cite{bierman2014understanding} (TS)
commonly replaces JavaScript in new or large codebases.
TS is transpiled to JS, it's a superset of JS
\footnote{Any JS program is also a TS program.}
and is statically typed.
Also other programming languages such as
Kotlin~\cite{KotlinToJs} can be transpiled to JS.

The three major execution engines for JS are V8~\cite{V8} (Chrome/Chromium),
SpiderMonkey~\cite{SpiderMonkey} (Firefox) and JavaScriptCore~\cite{JavaScriptCore} (Safari).
\footnote{Note that Edge is now based on Chromium and therefore
also uses V8.}
Among them, V8 is the underlying engine used in other
common JS execution environments on the server side, 
namely Node.js~\cite{NodeJS} and Deno~\cite{Deno}.


WebAssembly (Wasm)~\cite{Haas2017,WasmSpecification} is an alternative runtime
inside browsers.
The Wasm virtual machine (VM) is available in all major
browsers and can be used to run code either written directly in Wasm
or compiled from other languages.
\footnote{Wasm is also supported in Node.js on the server side, however the
code loading process is slightly different.} 
The latter is the
most common case, with source languages like C/C++, Rust,
Kotlin, Go, etc. C/C++ and Rust are low level languages
which allow for fine grained control of the memory
and do not rely on GC. Emscripten~\cite{Zakai2011} is the primary 
tool for compiling C/C++ to Wasm, using Clang compiler,
which is based on the Low Level Virtual Machine
(LLVM) architecture~\cite{LLVM2004}.
Rust\footnote{Rustc compiler internally also uses LLVM.} also supports compilation for whole application to Wasm
through Emscripten, using the \texttt{wasm32-unknown-emscripten}
compilation target.
However, currently the preferred way is to use
the \texttt{wasm32-unknown-unknown} target, which
compiles to Wasm directly without the need of Emscripten
and produces smaller binaries.\footnote{Normally,
Emscripten is used to port existing application instead 
of building libraries. It indeed 
provides a large standard library, 
containing things such as TCP sockets, 
file I/O, multithreading, openGL etc. that are needed in
standalone applications.
Rust is a new programming language compared to
old C and C++ therefore not so many application were yet 
written before Wasm was created. 
Thus, the preferred usage of Rust code compiled to Wasm
is to write libraries with bindings exposing the
compiled Wasm module to JS in the Browser.}
The Rust to Wasm standard tool chain includes 
wasm-pack~\cite{WasmPack} and wasm-bindgen~\cite{WasmBindgen}.
By using these, the compilation creates a Wasm 
module together with the JS bindings, the ``glue'' code
needed for interoperability, exporting the functionalities
so that are easily callable from JS. The TS type declarations 
for the JS generated code are also created.
While C/C++ and Rust compile down to native code, Kotlin normally executes on the
Java Virtual Machine (JVM). It is a high level language with GC,
but it can be compiled to Wasm thanks to the WasmGC proposal
and its support in common execution environments~\cite{WasmGCProposal, WasmGCinV8}.
Go similarly is a garbage collected language~\cite{GoGarbageCollector}.
However the compilation to Wasm~\cite{GOWasm} 
includes a GC in the compiled code itself, because WasmGC support doesn't
provide certain assurances that Go code expects from its own GC.\footnote{Specifically, the CG should not move memory around while cleaning the unreachable objects.}
For sake of completeness, we mention also the AssemblyScript~\cite{AssemblyScript} language, 
which is a subset of TS that statically compiles ahead of time to Wasm.
Being a subset of TS, AssemblyScript opens up possibilities for
interoperability and sharing of code between the two languages.
It got attention and traction in the community as web developers 
can write in a familiar syntax and easily compile optimised Wasm.

\subsection{Cryptography in the Browser}\label{sc:webcrypto-api}

There are several libraries in JS to provide cryptographic
operations.
However, those libraries present a lot of issues:
\begin{itemize}
    \item Assure constant time execution is hard. JS engines are constantly changing and tuning how they optimize code, leading to a variety of ever changing expectations for how the code will actually run
    \item In JS all numbers are 64-bit double precision floating point as specified in the IEEE 754 standard. This means that representation of integers is exact until $2^{53} - 1$.
    \item A good source of entropy is missing.
    \item Implementors are skilled JS programmers but not skilled cryptographers. Further, as mentioned in \cref{sc:abstract-to-real}, it is better to re-use basic primitive implementation that are likely more robust. 
\end{itemize}

The Web Crypto API, a World Wide Web Consortium (W3C) standard~\cite{WebCryptoAPISpecification}, 
provides basic cryptographic support in browsers.
The specification has been implemented by all major browsers
and is accessable from JS. 
Node.js and Deno for server-side JavaScript also implement the specification~\cite{NodeJsWebCryptoAPI, DenoWebCryptoAPI}.
The goal of the specification
is to provide a common interface to the underlying 
cryptographic primitives. Also, it provides calls to
a secure source of randomness. 
The API is asynchronous and it is implemented by each
browser providing native support. This means that the
operations are executed in constant time and can
utilize the hardware acceleration and advanced
security mechanism otherwise unavailable to JS, as well as
more precise arithmetic.
\footnote{For example, Chromium is internally calling BoringSSL for the cryptographic operations~\cite{ChromiumWebCryptoAPIImplementation}}
However, we note that the specification itself doesn't mandate
constant time execution of the operations.
Overall, the specification should aim to 
remove the need of aforementioned cryptographic JS libraries,
but support is missing for some new standard cryptographic objects. 
For example, for elliptic curves based cryptography, currently
the API supports P-256, P-386 and P-512 curves~\cite{WebCryptoAPICurvesSupport}.
Howerver, the ``secure curves''~\cite{WebCryptoAPISecureCurvesDraft,WebCryptoAPISecureCurvesExplainer}
are not yet part of the standard, although already recommended by
the Crypto Forum Research Group (CFRG) from the Internet Research
Task Force (IRTF) in 2016 and 2017~\cite{RFC7748IRTF, RFC8032IRTF}
and made part of the Federeal Information Processing Standard (FIPS) in 2023~\cite{SecureCurvesNIST}.

When compiling cryptographic code to Wasm in a browser environment,
all cryptographic operations should be done through bindings to
the Web Crypto API. Wasm itself doesn't provide any constant time
guarantee. However, some research were made to add such semantic
to the language~\cite{CTWasm, gu2023constanttimewasmtimerealtime}
and a Wasm constant-time proposal exists in the Wasm specification
repository~\cite{WasmCTProposal}.

\subsection{CGKA implementations}\label{sc:CGKA-implementations}

CGKA is a major component of the SSF scheme (\cref{sc:SSF}).
It's also a core component in MLS as seen in \cref{sc:CGKA}.
Indeed it is normally implemented as part of libraries developing
the full MLS specification.
To the best of our knowledge, the open source available libraries
providing MLS/CGKA are:
\begin{itemize}
    \item OpenMLS, available in multiple languages but not production-ready.
    \item Java BouncyCastle includes a CGKA only library.
    \item AWS-Lab Rust library ``mls-rs'', a full implementation of MLS sponsored by Amazon Web Services (AWS). 
\end{itemize}

Other minor implementation are available, but are mostly broken or outdated.
Thanks for the support for Wasm builds, ``mls-rs'' can be used in the browser.
Out of all the options available, ``mls-rs'' seemed the most promising:
we note that the library is developed by some of the authors of the MLS IETF
specification itself and is also integrated in the Android Open Source Project
code base\footnote{\url{https://android.googlesource.com/platform/external/rust/crates/mls-rs/}}.
The library provides crypto agility, meaning that the cryptographic
primitives can be easily swapped out for new ones.
For the Wasm build, the library uses bindings to the Web Crypto API 
(\cref{sc:webcrypto-api}) which act as a cryptographic operations provider.
As mentioned above, this way the library assure a safe execution of the underlying
algorithms and a good source of entropy inside the Browser. 
The bindings for the Web APIs are available in Rust 
through the ``web-sys'' crate,
\footnote{Crate is the name for a Rust library.}
which is procedurally generated from WebIDL language
\footnote{WebIDL is the interface description language used to define the Web APIs, describing the data types, interfaces, properties and methods and all other components that make up the API itself.} 
used in the API specifications~\cite{WebSys}, assuring
that they are up to date and correct.
Furthermore, while the SSF theoretical construction
assumes only the usage of CGKA without MLS (and thus the security proof
is simplified), we note that in practice MLS itself is needed.
More details are given in \cref{ch:ssf}.

\section{Cloud Storage: from an Abstract Model to Real Systems}\label{sc:cloud-storage}

Before explaining in detail the complete setup of the project,
we also need to deal with how to access the cloud storage provider,
which is the other core component in the SSF scheme construction.

In SSF, all cloud storage systems are abstracted away by assuming
that some operations are available to write and read data to a virtually
infinite storage system always available.
Deletion of files is not assumed to be secure, meaning that it is not guaranteed to happen faithfully and completely.
The above assumptions are well founded, given that a cloud storage provider
will normally provision new hardware as needed (in advance) to accomodate the
increased capacity request.
Also, a client writing in a public cloud doesn't have any control of that file anymore.
Furthermore, cloud providers state in their Service Level Agreement (SLA) that
with very high probability the service is up and running for more than a certain percentage of time or a refund is payed out to the clients
\footnote{e.g.\ Amazon Simple Storage Service (Amazon S3) starts to pay a refund to clients when the monthly uptime percentage is less than 99.9\%}
and that the durability of content uploaded is at least a certain percentage.
\footnote{As an example again, Amazon S3 is designed for 99.999999999\% durability}
The way cloud providers can meet those requirements is through replication, where the content is replicated at least (normally) 3 times, and in different geographical zones, to protect against widespread failures.
The content is automatically monitored and re-replicated in case some storage devices are failing. 
Furthermore, it is well known that in most case deleting a file from a disk doesn't delete the content but only marks that disk space as free without zerooing out the bits. 
Other reasons why we cannot assume that cloud storages perform secure deletion, at a higher level, but still caused by the same idea of marking some memory as free without actually deleting it,
happens in replicated storages, if the replicated storage is eventual-consistent, meaning that in case of a network partition the two parts of the system continue to work indipendently.
In such a scenario, if while the network is partitioned a side of the storage receives a deletion operation for a certain file, upon restoration,
the nodes still owning the data would try to replicate them to the other side.
Therefore, instead of deleting the information, the distributed data store creates a (usually temporary) tombstone record to keep track and eventually perform the deletion on all other nodes as well upon reconciliation.

\subsection{Object Storage}  
Examples of object storage solutions are Amazon Web Services (AWS)
Simple Storage Service (Amazon S3), Microsoft Azure Blob Storage, Google Cloud Storage (GCS).
This type of storage has all the characteristics aforementioned and can 
therefore be the actual technology used in the implementation. 
Further, it is called object storage as it deals with objects and not files, 
where an object can be very large (e.g.\ in Amazon S3 up to 5GB).
However, objects can only be written all at once
and update or delete them creates new versions of the entire data.
Compared to a normal file system, object storage has therefore a simpler API.

Other solutions could be used to implement the storage layer,
like a distributed file system.
However, object storage seems to be the most appropriate
solution as data is written encrypted by the client and therefore
there is no concept of file in-place modification.
Also, cost wise, object storage is normally very cheap.

\subsection{How do users access shared data?}
Cloud storage providers are normally accessed through an
identity system. The clients register to the cloud provider and
through their accounts they can store and load data.
Normally only accounts owning the data itself can access it.
However, some storage solutions allow for world-wide public access 
to the data store owned by one account,
\footnote{This is the case of Amazon S3 and all other aforementioned object storage solution.} 
although this is not recommended for sensitive data.
Note that in case each client have its own account,
some practical questions arise:
\begin{itemize}
    \item How to share data between clients?
    \item How to manage the access control?
    \item How to manage the billing?
\end{itemize}
Further, our system should be able to integrate with multiple
cloud storage providers. We do not want to force the users to
create accounts on a specific provider, rather this should be
transparent to them.
To this end, we abstract the cloud storage provider itself
and create a server component, called Gateway, which will manage
the access to the cloud storage provider, acting as a proxy
between clients and the cloud storage provider(s).
The Gateway do not only solve the practical issues above, but also
allows for expected security properties of the system to hold.
For example, by performing access control, the Gateway can ensure
that only members of the group can access the data in their
specific shared folder. Although data is E2EE, it is still
good practice to avoid non members freely download the data,
as the read operations on the storage are also billed.
Further, by disallowing arbitrary writes, the Gateway
protects the data from being overwritten by a malicious external user.
Although availability is out of scope in the SSF scheme,
it is practical required in any real-world system.

\section{Get Our Hands Dirty: The Tech Stack}

\subsection{Clients}

The client code is written in TypeScript.
Some of the code is written in Rust and compiled to Wasm.
The CGKA library of choice is ``mls-rs'' from AWS-Lab.
Those technology can all run in the browser. Specifically,
the resulting JavaScript and Wasm code can be bundled
together via Webpack, a common tool to prepare
the code for the browser.
The project includes a folder showcasing the setup with
Webpack.
However, to simplify developing and allow fast iterations and easier benchmarking,
we use Node.js as execution environment at first.
We develop therefore a command line interface (CLI) to test the code.
This way, we avoid the creation of a user
interface (UI)
\footnote{The UI is considered out of scope for this thesis given the short time frame, but it is a natural extension of the project.}
while still testing out the core protocol. 
As noted in \cref{sc:browser-runtimes}
Node.js internally uses V8, which allows us to assume
similar results in terms of performance and memory usage
when running the code in Chrome. It also provides
an implementation of the Web Crypto API.
However, we needed to patch the ``mls-rs'' library
to allow compatibility with Node.js.
\nd{cite the PR}
We only change how the library is binding the Web Crypto API.
We provider a JS snippet to load the Web Crypto API from
\texttt{node:crypto} instead of the \texttt{window.crypto} global
object from the browser. All the Rust code remains
unchanged. The library contains also another JS snippet
to use \texttt{Date.now()} function, which is written in
ES6 syntax, which Node.js doesn't support.
We therefore add a commonJS compliant re-implementation
of the same snippet which is used
when compaling for Node.js.
For the Rust code that is compiled to Wasm, we make use of
``wasm-bindgen-test'' to test the resulting Wasm code in 
Node.js, as well as in headless browsers.


\subsection{Servers}

Servers are developed in Rust.






