\chapter{Secure Shared Folder}\label{ch:ssf}

In this chapter we describe in details the Secure Shared Folder (SSF) implementation.
We detail the cryptographic implementation and usages of supporting
libraries, as well as their implications in terms of runtime execution
and the challenges we encountered. 
We provide a description of how the implementation diverges from
the paper's primitives pseudocode and the reasons behind them.
While implementing these undetying primitives indeed, we also discovered 
and fixed a few major issues, which caused failures in the 
synchronization of the cryptographic state among clients.
We detail our findings in~\cref{sc:correcting-primitives}, and we provide
the corrected pseudocode in the appendix of this document. 
In this context, we also discuss the state synchronization and rollbacks
in the SSF scheme~\cref{sc:state-sync-rollbacks}, where we also discuss
in details the importance of modelling precisely the
interactions between clients and servers. 
In~\cref{sc:MLS-enhancements} we also
propose enhancement to the MLS library and in general we detail
what features MLS implementations should provide to better support
the implementation of new cryptographic primitives on top of
CGKA and/or MLS.
Finally, we provide our enhancement proposals to the primitives, 
such that they can better model the different client entities with respect
to their cryptographic state (\cref{sc:DKR-enhancements}).



\section{SSKG}\label{sc:ssf-sskg}

The scheme uses SSKG, which are introduced in \cref{sc:SSKG}.
However, there is no browser-compatible implementation available to the best of our knowledge.\footnote{We found a reference Go implementation~\cite{SSKGGo}, which was used as reference together with the pseudocode in~\cite{ESORICS:MarPoe14}}
We therefore re-implement the pseudocode in~\cite{ESORICS:MarPoe14} using TypeScript.
The choice of the tree-based implementation is motivated both by its efficiency
compared to the numerical construction and because it only uses common cryptographic
primitives, such as hash functions, PRGs or block ciphers, which are supported by the Web Crypto API.
More importantly for the practical use case, is the added \texttt{SuperSeek}
functionality, which is a generalisation on top of the \texttt{Seek} procedure.
While \texttt{Seek} can only calculate an output starting from the initial state, 
\texttt{SuperSeek} can start the forward derivation from an arbitrary point of the sequence.
This allows the usage of SSKG in double key regression\cref{ssc:DKR},
as we can just share the state of the SSKG in the forward chain from 
the epoch we want to give access to. 
We pay more in terms of additional bytes sent over the wire, as the state
in the tree-based construction requires up to $O(log(n))$,
where $n$ is the number of elements in the sequence generated by the SSKG.

The subfolder \texttt{ssf-client/src/protocol/sskg/} of the project contains the code for the SSKG module.
The file \texttt{sskg.ts} specify the functionalities exported by an instance of the object.
The file \texttt{treeSSKg.ts} contains the implementation of the tree-based SSKG
as a TypeScript class \texttt{TreeSSKG}. The code unit tests are provided in \texttt{test/treeSSKG.test.ts} file,
with some additional randomized tests for further validation of the code.
The \texttt{TreeSSKG} class maintains the state internally in fields:
\begin{itemize}
    \item A read-only \texttt{name}, used to identify the SSKG instance and helpful in debugging.
    \item A read-only \texttt{totalNumberOfEpochs}, a positive integer representing the total number of elements derivable from this TreeSSKG instance.
    \item A mutable \texttt{stack} field, a stack data structure, i.e. a JS array, storing the internal state of the SSKG as detailed in~\cite{ESORICS:MarPoe14}. The stack is used as a more convenient and performant way of representing a pre-order traversal on the tree structure on which the SSKG is based on. Each element of the stack is a tuple of the form $[s, h]$, where $s$ is an element in the pseudo-random sequence, stored as a byte array of type \texttt{ArrayBuffer}, while $h$ is the height of the node storing $s$ in the tree.  
\end{itemize}

The implementation required the following cryptographic operations:
\begin{itemize}
    \item Generate a starting element (seed) for the SSKG.
    \item A pseudo-random function (PRF) to derive the next element in the sequence. We make use of the HKDF functionality exposed by the Web Crypto API with \texttt{subtle.deriveKey}, using SHA-256 as the hash function. Internally HKDF uses HMAC, however due to limitations imposed on the \texttt{CryptoKey} usages, we cannot directly use HMAC designated keys. To overcome this issue we   
\end{itemize}

This element is generated using the Web Crypto API \texttt{subtle.generateKey} function to create an HMAC SHA-256 symmetric key which is then exported to raw bytes using \texttt{exportKey}.

The Web Crypto API provides the necessary cryptographic primitives
through the \texttt{subtle} object, with the functions
\texttt{generateKey} and \texttt{deriveKey},
which support HMAC and HKDF algorithms.
We recall that the Web Crypto API representation for a
key, is made through the \texttt{CryptoKey} JS object,
which is a wrapper around the actual key material.
This object contains information about the key, such as
whether the key is a symmetric or asymmetric key, in the latter
case if it is private, the algorithm for which the key is used
and whether it can be exported or not to e.g. raw bytes.
To implement the PRF, we would need to call the \texttt{deriveKey}
function passing a HKDF \texttt{CryptoKey} to derive a new key.
The new derived key will later be used to derive the next
element in the SSKG generated sequence. However,
a call to \texttt{deriveKey} cannot produce a \texttt{CryptoKey}
designated for HKDF, thus we cannot reuse the output for a new
key derivation directly. We resort to use HMAC keys,
as we notice that the HKDF internally uses HMAC.
To use the HMAC key in the key derivation, we export it
though \texttt{exportKey} to raw bytes, and then import it
back to a new \texttt{CryptoKey} designated for HKDF.
In this way the generation of a SSKG seed is done through a call
to \texttt{generateKey} to create an HMAC key, which is exported
and stored as raw bytes inside the stack data structure in the
\texttt{TreeSSKG} instance field. The PRF is then operating on the
raw bytes as explained above.

The code provides also utilities for serializing and deserializing 
the object to persist the state in the client's browser storage
as well as sending it to the other members in the group.
Since the internal state contains raw bytes, we use the concise binary 
object representation (CBOR)~\cite{rfc8949} standard to encode it. 
CBOR supports natively the encoding of byte arrays, which is not the case
for JSON.

\section{Generalised DKR implementation}\label{sc:DKR-implementation}


\section{State synchronization and rollbacks}\label{sc:state-sync-rollbacks}

\nd{Add details on the mls-rs library and the state management when using it for encryption of application messages.}

\section{Web Crypto API implementations: non-standard behaviours}\label{sc:Web-Crypto-API-implementations:-non-standard-behaviours}

AWS mls-rs library implementors made the library available to WASM compilation
targets. The support is experimental, and involves for now only the implementation
of the cryptographic operations needed to execute MLS on top of the Web Crypto API.
\footnote{For practicioners: the library abstracts the underlying cryptographic code in Rust traits which are implemented for different runtime environments/cryptographic providers. The one used for the WASM target is implemented in the crate \texttt{mls-rs-crypto-webcrypto}.}
While looking at the history commits of the library, we found out that
apparently the support for WASM builds was added only for Chrome runtime.
Indeed, integration tests are available
which prove the compatibility and avoid regressions in Chrome.
However, as seen in \cref{ch:setup} we use Node.js to develop our client
CLI to avoid dealing with a UI in our first implementation.
Thanks to our multi-runtime setup, we discovered the portability issue
hidden in the library and in the implementations of the Web Crypto API.

When using elliptic-curve crypto primitives, the mls-rs library is 
calculating the public key from the bytes of a DER-encoded~\cite{Kaliski2002ALG} private key 
by passing the bytes of the key to the Web Crypto API \texttt{importKey}
call~\cite{WebCryptoAPIImportKey}. In Chrome, the underlying BoringSSL~\cite{BoringSSL} on which the Web Crypto API 
are implemented on, calculates
the public part of the key which is appended to the bytes provided in input.
However, this behaviour is non-standard and is not supported in other
major Web Crypto API implementations:
\begin{itemize}
    \item In Node.js, the \texttt{importKey} call will just import the key into a \texttt{CryptoKey} JS object, without any error~\cite{WebCryptoAPICryptoKey}. However, the underlying bytes are not modified. The mls-rs library has a check to detect the missing public key part and will then throw a runtime error. No documentation is provided on this issue, and we discover it by debugging the library compiled in WASM, which is not a trivial task. In the end we resorted on reading the source code. Although the documentation states that the cryptographic support is only experimental, it would be beneficial to add more details on this problem.
    \item In Safari, the \texttt{importKey} call will throw an error.
    \item In Firefox, the \texttt{importKey} call will throw an error.
\end{itemize}

The only case we could develop a work-around for is Node.js. Since in this runtime
the public key is not calculated but at the same time the operation does not fail,
after importing the private key we can force the calculation of the public key with an additional
export/import operation to \texttt{jwk} back to \texttt{pkcs8}~\cite{JsonWebKey, rfc5958}.
Indeed, by performing this \texttt{exportKey} call to produce the \texttt{jwk}
representation of the key, the curves coordinates $x$ and $y$ are calculated.
Therefore, importing back the key in \texttt{pkcs8} format will produce the correct
public key. The code is published in a pull request to the mls-rs library~\cite{AWSNodeJSCodeContributions}.

We notice that the mls-rs uses the Web Crypto API in a way conforming to the
specification, as the definition of the DER-encoded private key does not
restrict to the usage of elliptic curves private key info which must 
contain the public key field, but only should~\cite{rfc5915}:
\begin{quote}
    publicKey contains the elliptic curve public key associated with
    the private key in question.  The format of the public key is
    specified in Section 2.2 of [RFC5480].  Though the ASN.1 indicates
    publicKey is OPTIONAL, implementations that conform to this
    document SHOULD always include the publicKey field.
\end{quote}
We further notice that mls-rs implementation use case seems legitimate,
as the public key calculation would require non-constant time code
to execute on top of a secret key. Therefore, we would propose
to change the Web Crypto API \texttt{importKey} specification to
explicitly require the public key calculation, so that all
implementations are aligned with the same behaviour. In this way,
the API would support more advanced cryptographic operations on top
of the existing support for elliptic curves cryptography, such as the ones
required while trying to compute a key pair from a symmetric private key.

\section{mls-rs: WASM build enhancements}\label{sc:MLS-enhancements}

While working with the MLS library we found out that the WASM target
is not fully production ready:
\begin{itemize}
    \item The library is not fully portable to all major browser (\cref{sc:Web-Crypto-API-implementations:-non-standard-behaviours}).
    \item Node.js is not supported. We add support for the compiled WASM to run in Node.js by adding bindings to the Web Crypto API implementation from \texttt{node:crypto} module of Node.js~\cite{NodeJsWebCryptoAPI}. We adapt the existing JS inline code included in the library to be compatible with the JS module resolution mechanism and syntax of Node.js~\cite{AWSNodeJSCodeContributions}.\footnote{For practitioners: JS modules evolved during the years, Node.js is using CommonJS modules, which is the standard outside browser runtimes. These modules are also used in the npm ecosystem (the package manager for JS). The syntax of CommonJS is using plain JS objects and functions (require) to perform import and exports. Browsers use currently ES modules, which were added in 2015 to the JS standard and widely adopted in 2020. ES modules use the \texttt{import} and \texttt{export} keywords to perform the same operations.}
    \item The client state is not persisted and only kept in memory for now. We propose a strategy to persist the client-side state in the browser's using the Indexed Database API~\cite{MlsRsWebStorageProvider, IndexedDBAPI}.
    \item The X.509 certificates are not supported to manage identities of the users. This imposes the usage of Basic Credentials, which lower the security of the implementation as those credentials are not validated while performing group operations. We propose to add support for X.509 certificates in the library~\cite{MlsRsX509Certificates}. Our proposal aims to support the use case where the CA certificate is embedded in the client code or where the CA certificate is loaded at runtime through a different mechanism. This use case could be applied for example in a corporate environment.
\end{itemize}

\section{Correcting the primitives}\label{sc:correcting-primitives}

During the implementation and testing the GRaPPA constructions,
we found issues in the pseudocode descriptions as provided
in the original manuscript of \cite{GKP}.
In \cref{sc:GRaPPA-bugs} we provide the full explanations of the bugs, 
and the corrections we applied. 
We also discuss the implications of our corrections. 

The bugs we highlight in the following sections
derive from the underlying primitive not being expressive and
precise enough to model the interactions between distributed clients
playing either the role of admin or member.
In the generalized DKR primitive, the read/write (admin) 
and read-only (member) capabilities (see \cref{sc:DKR-implementation})
are not modelled at all in the mathematical definitions of the operations.
We explore the implications and attempt to correct
the primitive in~\cref{sc:DKR-enhancements}.

\subsection{GRaPPA bugs}\label{sc:GRaPPA-bugs}

While implementing the GRaPPA construction, we found out two bugs.
The bugs are both leading to the same issue: the state of the
sender admin and receivers admins can go out of sync after a certain
sequence of operations are executed.

\paragraph{Bug 1:} The admin operations \texttt{Rem}, \texttt{RemAdm} and
\texttt{RotKeys} require to advance the DKR state from the current epoch
$e_{c}$
to the next epoch $e_{r}$ and add blocks, meaning that new chains are created, 
either a forward, backward or both.
When an admin receives messages containing such operations and related
state updates, it will execute the \texttt{ProcCtrlAdmin} procedure.
In this procedure, the receiver admin will reconcile the local DKR state
with the state update received from the sender admin.
We note that the receiving admin at the time of receiving the message
has its local state synced up to epoch $e_{c}$.
While the \texttt{RemAdm} and \texttt{RotKeys} operations are sending 
the full DKR state, \texttt{Rem} is just sending an extension of the current state. 
This extension was however ignored by the receiving admin, 
which would just get out of sync. We adjusted the \texttt{ProcCtrlAdmin}
procedure to also handle the case where an extension is sent,
and process it. To this end, the receiving admin calculates the
interval of its complete local DKR state, from epoch 0 to the current
local epoch $e_c$. Then it will extend the interval with the extension
received from the sender admin, thus having access to the state up to
the epoch $e_r$. Finally, the resulting interval is used to override the
local DKR state of the admin. 
We notice that the sender admin might still
have access to a bigger state than the receivers,
as receiving admins are forced to shorten their last backward chain
to epoch $e_r$ to correctly process the extension.
However, the admin group will still be able to progress forward
to a new epoch $e_{r+1}$, and maintain all the local views of the DKR state
synchronized. We distinguish two cases, depending on which admin is
executing the next operation:

\begin{itemize}
    \item If the next operation is performed by the same sender admin,
the state of all other admins will be updated with a new extension or with
a full state update, depending on the operation.

\item When the next operation is performed by a different sender admin,
then a new backward chain will be started. The new initial element will 
be sent in the extension and the admin still holding the larger state
will also shrink its state to the epoch $e_r$. 
Then the extension will be processed, so the new initial element
will be added to start a new backward chain, and the state will be
synchronized up to epoch $e_{r+1}$. All other receiving admins will
also process the extension and will be able to progress to the new epoch,
following the same procedure, with the detail that the last backward chain
has already been shortened to epoch $e_r$ in the previous operation.

\end{itemize}

The implications of this na\"ive approach, where admins use the same
mechanism of members to extend their local state, is that in case admins
alternate in performing operations, at each operation a new backward chain
will be started, resulting in an average space complexity of $O(n)$ 
for the initial elements.
Another possible solution would be to send the DKR state also
in case of a \texttt{Rem} operation. This would instead 
increase the bandwidth usage.

\paragraph{Bug 2:} The second bug is caused by an optimization,
with which admins spare on the bandwidth usage while sending the new DKR state
from a sender admin to the receiver admins.
An admin performing the \texttt{Add}, \texttt{UpdAdm} or \texttt{AddAdm}
operation needs to advance the global
epoch of the group and only release a new forward chain element.
This is performed by calling the \texttt{progress} procedure on the current
DKR state. No block is needed, as we only need to disallow
the new member of the group from generating keys corresponding
to epochs before the one in which he joined. Therefore, the receiving
admins would need only to call progress on their local copy of the DKR
global state and should be able to calculate the new forward chain element.
However, recalling the generalisation of the DKR construction,
we might be exactly at the epoch corresponding to a maximum chain length,
either for the latest forward chain, the latest backward chain or both.
In this case, the sender admin would need to generate a new random initial 
element for the new chain(s). If also the receivers randomly sample new 
initial elements in their local state, all admins would be out of sync.
The fix we have implemented is to also send an extension in those cases,
with the same implications as seen above for the other bug.


\subsection{DKR enhancements}\label{sc:DKR-enhancements}

In the discussion on the implications of the adjustments to GRaPPA in ~\cref{sc:GRaPPA-bugs},
we have seen how the admins can get out of sync.
We highlight that the current na\"ive solution still
has some drawbacks, as the state of all admins is not
fully synchronized, where an admin maintains a larder state
comprising all the latest backward chain elements other admins
lost access to.

The key point we want to stress is that the DKR primitive
does not capture the distinction between a full DKR state,
with read/write access, and a partial DKR state, with read-only access
together with the associated operations.
We claim that this distinction is crucial to understand the discrepancy
existing between the state of two clients with different capabilities,
namely admins and members:

\begin{itemize}
    \item A DKR full state, which we will simply call DKR state,
    comprises: 
    the current epoch $e_{max}$, 
    the complete list of backward chains
    and the complete list of forward chains,
    from epoch $0$ to epoch $e_{max}$ and 
    a parameter $N$ indicating the maximum length of a chain.
    Each chain is stored from its initial element. 
    In particular, the latest backward chain
    is stored from the initial element of the element sequence order,
    which corresponds to the latest possible element which will be released.
    The state thus contains all the elements to derive the state up to
    the current $e_{max}$ epoch and possibly beyond. 
    In case the latest forward chain is not fully used, i.e.,
    the latest $N$-element was not yet released, and the backward
    chain is not also fully used, i.e., the element
    corresponding to $e_{max}$ is not the initial element of the chain,
    it is possible to compute new elements of both
    chains, which can be used to derive keys for the next epochs.
    \item An interval state is a subset of the DKR state, which
    allows deriving keys for the epochs represented in the interval.
    It comprises: the epoch interval with the starting ($e_{left}$) and ending ($e_{right}$) 
    epochs, with the constraint $0 \leq e_{left} \leq e_{right} \leq e_{max}$,
    where $e_{max}$ is the current epoch of the DKR state from which the interval
    state is extracted. Also, it includes the slices of the backward
    and forward chains corresponding to the epochs in the interval.
    More precisely, the forward (respectively backward) chain from which the
    element for the epoch $e_{left}$ (respectively $e_{right}$) can be derived
    is shrunk to that element, disallowing the derivation of any key outside the epoch interval.
\end{itemize}

When we consider a shared DKR state among multiple clients,
as in the implementation of GRaPPA, it becomes clear
that an operation to extend a copy of the DKR state is
missing in the scheme.
We therefore propose to add the following operation to the DKR
primitive, of which we provide an informal description:

\begin{itemize}
    \item \texttt{CreateFExt($st$, $l$)}, on input the DKR state $st$ and an epoch $l$,
    returns a full-extension $fext$ or error. The full-extension is an interval state
    starting at epoch $l$ and ending at the current $e_{max}$ epoch of the DKR state
    $st$, similarly to an extension. However, in a full-extension we do not 
    specify the ending epoch, as the latest backward chain included in the interval
    could give access to the state beyond the current epoch. Practically speaking,
    the full-extension is an extension where the latest backward chain is not shrunk.
    We further highlight that full-extensions always comprises the latest current epoch,
    as they serve the specific purpose of extending the state of a replica of the DKR state.
    Also, a full-extension contains the current epoch $e_{max}$.

    \item \texttt{ProcessFExt($st$, $fext$)}, on input the DKR state $st$ and a full-extension $fext$,
    returns the updated DKR state $st'$ or error. The procedure processes the full-extension
    $fext$ on the provided $st$ as the existing \texttt{ProcExt} operation processes an extension $ext$
    on a provided interval state $int$, with the additional
    operation of setting the state current epoch $st.e_{max}$ to current epoch contained
    in the full-extension $fext.e_{max}$.
    
\end{itemize}

Equipped with the new full-extension entity, its creation and processing operations,
the generalised DKR primitive can be now used in the GKP instantiation
GRaPPA to solve the bugs from \cref{sc:GRaPPA-bugs}.
Instead of applying the na\"ive solution, where the admins
extend their local state as members, or they always send to each
other the full state, the admins can now send full-extensions
among them for all operations that require to progress the DKR state,
i.e. all admin operations.
This will allow them to keep their local copy of
the DKR state synchronized, with an optimal constant bandwidth 
consumption, thus serving the purpose of correcting the 
protocol and clarify how to minimize the bandwidth consumption.
